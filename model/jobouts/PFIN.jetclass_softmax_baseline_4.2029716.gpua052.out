Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
A conda environment has been detected CONDA_PREFIX=
/sw/external/python/anaconda3 
anaconda3_gpu is loaded. Consider running conda deactivate and reloading it.

Currently Loaded Modules:
  1) cue-login-env/1.0   4) gcc/11.2.0    7) openmpi/4.1.2
  2) default             5) cuda/11.6.1   8) cudnn/8.4.1.50
  3) modtree/gpu         6) ucx/1.11.2    9) anaconda3_gpu/4.13.0

 

/u/avroy/UQ_XAI_Studies/models/PFIN_UQ/model/jobsub
total 508K
-rw-rw----+ 1 avroy delta_bbhj  925 Mar 31 16:58 jobsubmitter_dummy.py
-rw-rw----+ 1 avroy delta_bbhj 5.2K May 27 21:16 jobsubmitter_jetclass_1gpu.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 27 21:04 jobsubmitter_jetclass_1gpu.py~
-rw-rw----+ 1 avroy delta_bbhj  743 May 19 01:04 jobsubmitter_jetclass.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 18 13:59 jobsubmitter.py
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 19 13:52 UQPFIN-jetclass-run-jetclass20M_0_baseline.slurm
-rw-------+ 1 avroy delta_bbhj  973 May 19 00:59 UQPFIN-jetclass-run.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_9.slurm
-rw-------+ 1 avroy delta_bbhj  969 May 15 09:44 UQPFIN-run.slurm
/projects/bbhj/avroy/UQ_XAI_Studies/models/PFIN_UQ/model
total 184K
drwxr-s---+ 2 avroy delta_bbhj  12K May 28 01:44 jobouts
drwxr-s---+ 2 avroy delta_bbhj  16K May 27 21:18 jobsub
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 18 21:37 PFINDataset.py
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 15 14:46 PFINDataset.py~
drwxrws---+ 2 avroy delta_bbhj 4.0K May 18 21:38 __pycache__
drwxrws---+ 2 avroy delta_bbhj  16K May 27 23:34 trained_model_dicts
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:09 trained_model_dicts_old
drwxrws---+ 2 avroy delta_bbhj  24K May 27 23:53 trained_models
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:10 trained_models_old
-rw-rw----+ 1 avroy delta_bbhj  23K May 18 15:52 train_mod.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 27 21:06 train.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 18 13:50 train.py~
-rw-rw----+ 1 avroy delta_bbhj  13K May  5 09:36 UQPFIN.py
job is starting on gpua052.delta.ncsa.illinois.edu
# conda environments:
#
base                     /sw/external/python/anaconda3
toptagger_env         *  /u/avroy/.conda/envs/toptagger_env

/sw/external/python/anaconda3_gpu/bin/python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UQPFIN                                   [1, 10]                   --
├─Sequential: 1-1                        [60, 64]                  --
│    └─Sequential: 2-1                   [60, 100]                 --
│    │    └─Linear: 3-1                  [60, 100]                 1,200
│    │    └─ReLU: 3-2                    [60, 100]                 --
│    └─Sequential: 2-2                   [60, 100]                 --
│    │    └─Linear: 3-3                  [60, 100]                 10,100
│    │    └─ReLU: 3-4                    [60, 100]                 --
│    └─Sequential: 2-3                   [60, 64]                  --
│    │    └─Linear: 3-5                  [60, 64]                  6,464
│    │    └─ReLU: 3-6                    [60, 64]                  --
├─Sequential: 1-2                        [1770, 64]                --
│    └─Sequential: 2-4                   [1770, 128]               --
│    │    └─Linear: 3-7                  [1770, 128]               640
│    │    └─ReLU: 3-8                    [1770, 128]               --
│    └─Sequential: 2-5                   [1770, 128]               --
│    │    └─Linear: 3-9                  [1770, 128]               16,512
│    │    └─ReLU: 3-10                   [1770, 128]               --
│    └─Sequential: 2-6                   [1770, 64]                --
│    │    └─Linear: 3-11                 [1770, 64]                8,256
│    │    └─ReLU: 3-12                   [1770, 64]                --
├─Sequential: 1-3                        [60, 64]                  --
│    └─Sequential: 2-7                   [60, 128]                 --
│    │    └─Linear: 3-13                 [60, 128]                 9,728
│    │    └─ReLU: 3-14                   [60, 128]                 --
│    └─Sequential: 2-8                   [60, 128]                 --
│    │    └─Linear: 3-15                 [60, 128]                 16,512
│    │    └─ReLU: 3-16                   [60, 128]                 --
│    └─Sequential: 2-9                   [60, 64]                  --
│    │    └─Linear: 3-17                 [60, 64]                  8,256
│    │    └─ReLU: 3-18                   [60, 64]                  --
├─Sequential: 1-4                        [1, 10]                   --
│    └─Sequential: 2-10                  [1, 64]                   --
│    │    └─Linear: 3-19                 [1, 64]                   4,160
│    │    └─ReLU: 3-20                   [1, 64]                   --
│    └─Sequential: 2-11                  [1, 100]                  --
│    │    └─Linear: 3-21                 [1, 100]                  6,500
│    │    └─ReLU: 3-22                   [1, 100]                  --
│    └─Sequential: 2-12                  [1, 100]                  --
│    │    └─Linear: 3-23                 [1, 100]                  10,100
│    │    └─ReLU: 3-24                   [1, 100]                  --
│    └─Linear: 2-13                      [1, 10]                   1,010
│    └─Softmax: 2-14                     [1, 10]                   --
==========================================================================================
Total params: 99,438
Trainable params: 99,438
Non-trainable params: 0
Total mult-adds (M): 48.13
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 4.81
Params size (MB): 0.40
Estimated Total Size (MB): 5.21
==========================================================================================
data type:  jetclass
m_logic, pt_logic, eta_logic:  AND AND AND
m1, m2, pt1, pt2, eta1, eta2:  0.0 10000.0 0.0 10000.0 -6.0 6.0
skip labels:  []
classes:  10
Epoch 0
Best Validation Accuracy: 0
Current Validation Accuracy: 0.7097491932655998
Current Validation Loss: 0.0003238635679295365
Saving best model based on accuracy
Epoch 1
Best Validation Accuracy: 0.7097491932655998
Current Validation Accuracy: 0.7214474048188468
Current Validation Loss: 0.0003110763230397026
Saving best model based on accuracy
Epoch 2
Best Validation Accuracy: 0.7214474048188468
Current Validation Accuracy: 0.7264721433398579
Current Validation Loss: 0.00030523645770615256
Saving best model based on accuracy
Epoch 3
Best Validation Accuracy: 0.7264721433398579
Current Validation Accuracy: 0.7306069854120011
Current Validation Loss: 0.0003007677529928683
Saving best model based on accuracy
Epoch 4
Best Validation Accuracy: 0.7306069854120011
Current Validation Accuracy: 0.7335262859603954
Current Validation Loss: 0.00029778359385999507
Saving best model based on accuracy
Epoch 5
Best Validation Accuracy: 0.7335262859603954
Current Validation Accuracy: 0.7346727967309437
Current Validation Loss: 0.00029664419494828016
Saving best model based on accuracy
Epoch 6
Best Validation Accuracy: 0.7346727967309437
Current Validation Accuracy: 0.7362960198768551
Current Validation Loss: 0.0002949556832573031
Saving best model based on accuracy
Epoch 7
Best Validation Accuracy: 0.7362960198768551
Current Validation Accuracy: 0.7470818249530166
Current Validation Loss: 0.00028484572047285834
Saving best model based on accuracy
Epoch 8
Best Validation Accuracy: 0.7470818249530166
Current Validation Accuracy: 0.7642224611064229
Current Validation Loss: 0.00026628149279207423
Saving best model based on accuracy
Epoch 9
Best Validation Accuracy: 0.7642224611064229
Current Validation Accuracy: 0.7667370813697503
Current Validation Loss: 0.00026332126163650533
Saving best model based on accuracy
Epoch 10
Best Validation Accuracy: 0.7667370813697503
Current Validation Accuracy: 0.775777608924776
Current Validation Loss: 0.00025317385399993634
Saving best model based on accuracy
Epoch 11
Best Validation Accuracy: 0.775777608924776
Current Validation Accuracy: 0.7718228470783735
Current Validation Loss: 0.00025697208049909915
Epoch 12
Best Validation Accuracy: 0.775777608924776
Current Validation Accuracy: 0.7666175281087725
Current Validation Loss: 0.0002640572649205034
Epoch 13
Best Validation Accuracy: 0.775777608924776
Current Validation Accuracy: 0.7807533256065577
Current Validation Loss: 0.0002475010202487274
Saving best model based on accuracy
Epoch 14
Best Validation Accuracy: 0.7807533256065577
Current Validation Accuracy: 0.7842969042708526
Current Validation Loss: 0.00024308357884723425
Saving best model based on accuracy
Epoch 15
Best Validation Accuracy: 0.7842969042708526
Current Validation Accuracy: 0.7784447971571336
Current Validation Loss: 0.00024972236826080376
Epoch 16
Best Validation Accuracy: 0.7842969042708526
Current Validation Accuracy: 0.785696027580287
Current Validation Loss: 0.00024163074532345
Saving best model based on accuracy
Epoch 17
Best Validation Accuracy: 0.785696027580287
Current Validation Accuracy: 0.7874823233750636
Current Validation Loss: 0.00023976445277905273
Saving best model based on accuracy
Epoch 18
Best Validation Accuracy: 0.7874823233750636
Current Validation Accuracy: 0.7641249176508135
Current Validation Loss: 0.000266213361510328
Epoch 19
Best Validation Accuracy: 0.7874823233750636
Current Validation Accuracy: 0.7841883559125591
Current Validation Loss: 0.00024327585422917016
Epoch 20
Best Validation Accuracy: 0.7874823233750636
Current Validation Accuracy: 0.7872897375780911
Current Validation Loss: 0.00024006639258083399
Epoch 21
Best Validation Accuracy: 0.7874823233750636
Current Validation Accuracy: 0.7825081073618297
Current Validation Loss: 0.00024534706910040913
Epoch 22
Best Validation Accuracy: 0.7874823233750636
Current Validation Accuracy: 0.7790135505367641
Current Validation Loss: 0.0002487379385512805
Epoch 23
Best Validation Accuracy: 0.7874823233750636
Current Validation Accuracy: 0.7895652513194629
Current Validation Loss: 0.00023740644166064307
Saving best model based on accuracy
Epoch 24
Best Validation Accuracy: 0.7895652513194629
Current Validation Accuracy: 0.7904041250377043
Current Validation Loss: 0.00023655875072945928
Saving best model based on accuracy
Epoch 25
Best Validation Accuracy: 0.7904041250377043
Current Validation Accuracy: 0.7906152190801002
Current Validation Loss: 0.00023630488644440365
Saving best model based on accuracy
Epoch 26
Best Validation Accuracy: 0.7906152190801002
Current Validation Accuracy: 0.787676409840584
Current Validation Loss: 0.0002391127207987542
Epoch 27
Best Validation Accuracy: 0.7906152190801002
Current Validation Accuracy: 0.7892436080273761
Current Validation Loss: 0.00023780867582145777
Epoch 28
Best Validation Accuracy: 0.7906152190801002
Current Validation Accuracy: 0.7906352279940714
Current Validation Loss: 0.00023618802474616263
Saving best model based on accuracy
Epoch 29
[[9.82351601e-01 1.13925540e-04 7.50560255e-04 2.34649563e-03
  6.36868645e-05 5.42074195e-05 6.89185830e-03 7.36304605e-03
  1.13519536e-05 5.31972400e-05]
 [3.23472358e-02 1.91709423e-03 2.22263843e-01 6.04190171e-01
  1.23882972e-01 6.27013560e-06 1.02190087e-02 2.30408739e-03
  2.86669703e-03 2.61705668e-06]
 [4.26145107e-06 1.71303345e-05 9.13181429e-06 3.30580519e-07
  1.34903280e-08 5.56482235e-04 4.22954645e-06 1.15740512e-07
  6.55588082e-08 9.99408126e-01]
 [3.77125340e-03 1.62545834e-02 5.66249043e-02 4.13162634e-03
  1.21068172e-02 8.38609576e-01 5.48666231e-02 1.29426038e-02
  3.46720102e-04 3.45354347e-04]
 [3.70723478e-06 1.21017347e-05 1.42566279e-08 2.13833019e-07
  4.81985063e-10 6.61476224e-04 8.70822714e-06 9.89289006e-08
  5.02996613e-08 9.99313474e-01]
 [1.31768972e-01 3.50536866e-04 5.89095056e-03 4.60819714e-02
  1.64700497e-03 6.94244402e-04 3.90095532e-01 4.23418373e-01
  4.94951564e-05 2.94443316e-06]
 [2.11304408e-02 1.00441165e-01 4.21810255e-05 1.04520330e-03
  5.55875317e-07 2.86261525e-07 8.76643240e-01 4.84047778e-04
  1.77485636e-04 3.53554606e-05]
 [4.57392103e-04 1.32631285e-05 1.95331592e-03 5.68852434e-03
  9.91146863e-01 8.31036323e-06 1.83564596e-04 3.53362644e-04
  1.95043423e-04 2.64803418e-07]
 [9.94386911e-01 4.26044517e-06 6.54278920e-05 5.44272916e-05
  7.81095423e-06 1.63916102e-05 3.09890322e-03 2.36269110e-03
  1.40934742e-06 1.76289927e-06]
 [1.32860325e-04 1.05226747e-04 2.22220458e-02 5.66517795e-03
  9.70287740e-01 1.80833808e-06 3.70937021e-04 5.59324748e-04
  6.54605392e-04 3.58745950e-07]
 [4.62534046e-03 1.04110470e-04 3.90272820e-03 1.17887207e-03
  4.17267997e-03 1.01966015e-03 3.12081367e-01 6.72843277e-01
  7.13201880e-05 6.65410312e-07]
 [9.87050891e-01 3.35837976e-05 2.98615807e-04 5.69143391e-04
  8.33418017e-06 1.48138470e-05 5.89654176e-03 6.10756408e-03
  1.48466233e-06 1.89233615e-05]
 [1.36411311e-02 1.40369160e-03 8.42462406e-02 2.22955570e-01
  6.41821206e-01 4.21117693e-05 8.15665536e-03 9.47510451e-03
  1.82506405e-02 7.72561725e-06]
 [2.72435113e-03 4.90053773e-01 5.55165857e-03 9.39060561e-03
  6.62148697e-04 9.61402839e-05 4.90607768e-01 1.77420327e-04
  7.15858536e-04 2.03576219e-05]
 [9.90402222e-01 2.97008719e-05 1.79768633e-03 2.54457464e-05
  5.34705407e-07 4.13014604e-05 4.56732418e-03 2.31444347e-03
  5.17553475e-04 3.03648616e-04]
 [1.07038375e-02 3.94136005e-04 2.69506183e-02 6.82876408e-02
  8.62553239e-01 2.10392664e-05 3.90997250e-03 4.21645027e-03
  2.29608938e-02 2.22967560e-06]
 [4.51960523e-06 3.34088236e-06 2.29083871e-05 2.05039687e-05
  3.89700763e-05 9.99747455e-01 5.92479546e-06 1.99603091e-05
  2.99504177e-06 1.33300084e-04]
 [7.71621168e-02 3.86242209e-05 1.43048307e-03 6.29504696e-02
  2.67149478e-01 4.46116930e-04 7.00520948e-02 2.94641275e-02
  4.91290122e-01 1.63671320e-05]
 [1.10689434e-03 3.38151585e-03 6.66497508e-03 9.71705595e-04
  3.67111730e-04 1.40833417e-05 2.27130135e-03 1.52218086e-03
  9.83671486e-01 2.87225921e-05]
 [4.85659111e-05 8.11325606e-07 8.81217966e-06 1.21380901e-06
  9.30074066e-07 9.81297076e-01 1.44545565e-05 9.34550371e-06
  2.91141009e-06 1.86159033e-02]]
[[9.82351601e-01 1.13925540e-04 7.50560255e-04 2.34649563e-03
  6.36868645e-05 5.42074195e-05 6.89185830e-03 7.36304605e-03
  1.13519536e-05 5.31972400e-05]
 [3.23472358e-02 1.91709423e-03 2.22263843e-01 6.04190171e-01
  1.23882972e-01 6.27013560e-06 1.02190087e-02 2.30408739e-03
  2.86669703e-03 2.61705668e-06]
 [4.26145107e-06 1.71303345e-05 9.13181429e-06 3.30580519e-07
  1.34903280e-08 5.56482235e-04 4.22954645e-06 1.15740512e-07
  6.55588082e-08 9.99408126e-01]
 [3.77125340e-03 1.62545834e-02 5.66249043e-02 4.13162634e-03
  1.21068172e-02 8.38609576e-01 5.48666231e-02 1.29426038e-02
  3.46720102e-04 3.45354347e-04]
 [3.70723478e-06 1.21017347e-05 1.42566279e-08 2.13833019e-07
  4.81985063e-10 6.61476224e-04 8.70822714e-06 9.89289006e-08
  5.02996613e-08 9.99313474e-01]
 [1.31768972e-01 3.50536866e-04 5.89095056e-03 4.60819714e-02
  1.64700497e-03 6.94244402e-04 3.90095532e-01 4.23418373e-01
  4.94951564e-05 2.94443316e-06]
 [2.11304408e-02 1.00441165e-01 4.21810255e-05 1.04520330e-03
  5.55875317e-07 2.86261525e-07 8.76643240e-01 4.84047778e-04
  1.77485636e-04 3.53554606e-05]
 [4.57392103e-04 1.32631285e-05 1.95331592e-03 5.68852434e-03
  9.91146863e-01 8.31036323e-06 1.83564596e-04 3.53362644e-04
  1.95043423e-04 2.64803418e-07]
 [9.94386911e-01 4.26044517e-06 6.54278920e-05 5.44272916e-05
  7.81095423e-06 1.63916102e-05 3.09890322e-03 2.36269110e-03
  1.40934742e-06 1.76289927e-06]
 [1.32860325e-04 1.05226747e-04 2.22220458e-02 5.66517795e-03
  9.70287740e-01 1.80833808e-06 3.70937021e-04 5.59324748e-04
  6.54605392e-04 3.58745950e-07]
 [4.62534046e-03 1.04110470e-04 3.90272820e-03 1.17887207e-03
  4.17267997e-03 1.01966015e-03 3.12081367e-01 6.72843277e-01
  7.13201880e-05 6.65410312e-07]
 [9.87050891e-01 3.35837976e-05 2.98615807e-04 5.69143391e-04
  8.33418017e-06 1.48138470e-05 5.89654176e-03 6.10756408e-03
  1.48466233e-06 1.89233615e-05]
 [1.36411311e-02 1.40369160e-03 8.42462406e-02 2.22955570e-01
  6.41821206e-01 4.21117693e-05 8.15665536e-03 9.47510451e-03
  1.82506405e-02 7.72561725e-06]
 [2.72435113e-03 4.90053773e-01 5.55165857e-03 9.39060561e-03
  6.62148697e-04 9.61402839e-05 4.90607768e-01 1.77420327e-04
  7.15858536e-04 2.03576219e-05]
 [9.90402222e-01 2.97008719e-05 1.79768633e-03 2.54457464e-05
  5.34705407e-07 4.13014604e-05 4.56732418e-03 2.31444347e-03
  5.17553475e-04 3.03648616e-04]
 [1.07038375e-02 3.94136005e-04 2.69506183e-02 6.82876408e-02
  8.62553239e-01 2.10392664e-05 3.90997250e-03 4.21645027e-03
  2.29608938e-02 2.22967560e-06]
 [4.51960523e-06 3.34088236e-06 2.29083871e-05 2.05039687e-05
  3.89700763e-05 9.99747455e-01 5.92479546e-06 1.99603091e-05
  2.99504177e-06 1.33300084e-04]
 [7.71621168e-02 3.86242209e-05 1.43048307e-03 6.29504696e-02
  2.67149478e-01 4.46116930e-04 7.00520948e-02 2.94641275e-02
  4.91290122e-01 1.63671320e-05]
 [1.10689434e-03 3.38151585e-03 6.66497508e-03 9.71705595e-04
  3.67111730e-04 1.40833417e-05 2.27130135e-03 1.52218086e-03
  9.83671486e-01 2.87225921e-05]
 [4.85659111e-05 8.11325606e-07 8.81217966e-06 1.21380901e-06
  9.30074066e-07 9.81297076e-01 1.44545565e-05 9.34550371e-06
  2.91141009e-06 1.86159033e-02]]
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
Best Validation Accuracy: 0.7906352279940714
Current Validation Accuracy: 0.792402015097726
Current Validation Loss: 0.00023417260769303768
Saving best model based on accuracy
Saving last model
