Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
A conda environment has been detected CONDA_PREFIX=
/sw/external/python/anaconda3 
anaconda3_gpu is loaded. Consider running conda deactivate and reloading it.

Currently Loaded Modules:
  1) cue-login-env/1.0   4) gcc/11.2.0    7) openmpi/4.1.2
  2) default             5) cuda/11.6.1   8) cudnn/8.4.1.50
  3) modtree/gpu         6) ucx/1.11.2    9) anaconda3_gpu/4.13.0

 

/u/avroy/UQ_XAI_Studies/models/PFIN_UQ/model/jobsub
total 508K
-rw-rw----+ 1 avroy delta_bbhj  925 Mar 31 16:58 jobsubmitter_dummy.py
-rw-rw----+ 1 avroy delta_bbhj 5.2K May 27 21:16 jobsubmitter_jetclass_1gpu.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 27 21:04 jobsubmitter_jetclass_1gpu.py~
-rw-rw----+ 1 avroy delta_bbhj  743 May 19 01:04 jobsubmitter_jetclass.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 18 13:59 jobsubmitter.py
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 19 13:52 UQPFIN-jetclass-run-jetclass20M_0_baseline.slurm
-rw-------+ 1 avroy delta_bbhj  973 May 19 00:59 UQPFIN-jetclass-run.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_9.slurm
-rw-------+ 1 avroy delta_bbhj  969 May 15 09:44 UQPFIN-run.slurm
/projects/bbhj/avroy/UQ_XAI_Studies/models/PFIN_UQ/model
total 184K
drwxr-s---+ 2 avroy delta_bbhj  12K May 28 01:44 jobouts
drwxr-s---+ 2 avroy delta_bbhj  16K May 27 21:18 jobsub
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 18 21:37 PFINDataset.py
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 15 14:46 PFINDataset.py~
drwxrws---+ 2 avroy delta_bbhj 4.0K May 18 21:38 __pycache__
drwxrws---+ 2 avroy delta_bbhj  16K May 27 23:34 trained_model_dicts
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:09 trained_model_dicts_old
drwxrws---+ 2 avroy delta_bbhj  24K May 27 23:53 trained_models
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:10 trained_models_old
-rw-rw----+ 1 avroy delta_bbhj  23K May 18 15:52 train_mod.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 27 21:06 train.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 18 13:50 train.py~
-rw-rw----+ 1 avroy delta_bbhj  13K May  5 09:36 UQPFIN.py
job is starting on gpua035.delta.ncsa.illinois.edu
# conda environments:
#
base                     /sw/external/python/anaconda3
toptagger_env         *  /u/avroy/.conda/envs/toptagger_env

/sw/external/python/anaconda3_gpu/bin/python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UQPFIN                                   [1, 8]                    --
├─Sequential: 1-1                        [60, 64]                  --
│    └─Sequential: 2-1                   [60, 100]                 --
│    │    └─Linear: 3-1                  [60, 100]                 1,200
│    │    └─ReLU: 3-2                    [60, 100]                 --
│    └─Sequential: 2-2                   [60, 100]                 --
│    │    └─Linear: 3-3                  [60, 100]                 10,100
│    │    └─ReLU: 3-4                    [60, 100]                 --
│    └─Sequential: 2-3                   [60, 64]                  --
│    │    └─Linear: 3-5                  [60, 64]                  6,464
│    │    └─ReLU: 3-6                    [60, 64]                  --
├─Sequential: 1-2                        [1770, 64]                --
│    └─Sequential: 2-4                   [1770, 128]               --
│    │    └─Linear: 3-7                  [1770, 128]               640
│    │    └─ReLU: 3-8                    [1770, 128]               --
│    └─Sequential: 2-5                   [1770, 128]               --
│    │    └─Linear: 3-9                  [1770, 128]               16,512
│    │    └─ReLU: 3-10                   [1770, 128]               --
│    └─Sequential: 2-6                   [1770, 64]                --
│    │    └─Linear: 3-11                 [1770, 64]                8,256
│    │    └─ReLU: 3-12                   [1770, 64]                --
├─Sequential: 1-3                        [60, 64]                  --
│    └─Sequential: 2-7                   [60, 128]                 --
│    │    └─Linear: 3-13                 [60, 128]                 9,728
│    │    └─ReLU: 3-14                   [60, 128]                 --
│    └─Sequential: 2-8                   [60, 128]                 --
│    │    └─Linear: 3-15                 [60, 128]                 16,512
│    │    └─ReLU: 3-16                   [60, 128]                 --
│    └─Sequential: 2-9                   [60, 64]                  --
│    │    └─Linear: 3-17                 [60, 64]                  8,256
│    │    └─ReLU: 3-18                   [60, 64]                  --
├─Sequential: 1-4                        [1, 8]                    --
│    └─Sequential: 2-10                  [1, 64]                   --
│    │    └─Linear: 3-19                 [1, 64]                   4,160
│    │    └─ReLU: 3-20                   [1, 64]                   --
│    └─Sequential: 2-11                  [1, 100]                  --
│    │    └─Linear: 3-21                 [1, 100]                  6,500
│    │    └─ReLU: 3-22                   [1, 100]                  --
│    └─Sequential: 2-12                  [1, 100]                  --
│    │    └─Linear: 3-23                 [1, 100]                  10,100
│    │    └─ReLU: 3-24                   [1, 100]                  --
│    └─Linear: 2-13                      [1, 8]                    808
│    └─Softmax: 2-14                     [1, 8]                    --
==========================================================================================
Total params: 99,236
Trainable params: 99,236
Non-trainable params: 0
Total mult-adds (M): 48.13
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 4.81
Params size (MB): 0.40
Estimated Total Size (MB): 5.21
==========================================================================================
data type:  jetclass
m_logic, pt_logic, eta_logic:  AND AND AND
m1, m2, pt1, pt2, eta1, eta2:  0.0 10000.0 0.0 10000.0 -6.0 6.0
skip labels:  [8, 9]
classes:  8
Epoch 0
Best Validation Accuracy: 0
Current Validation Accuracy: 0.6757224410569017
Current Validation Loss: 0.0004338812053660217
Saving best model based on accuracy
Epoch 1
Best Validation Accuracy: 0.6757224410569017
Current Validation Accuracy: 0.6935540454260356
Current Validation Loss: 0.0004138995208327497
Saving best model based on accuracy
Epoch 2
Best Validation Accuracy: 0.6935540454260356
Current Validation Accuracy: 0.6997555541242191
Current Validation Loss: 0.00040687345843769506
Saving best model based on accuracy
Epoch 3
Best Validation Accuracy: 0.6997555541242191
Current Validation Accuracy: 0.7050065441956768
Current Validation Loss: 0.0004016432127415516
Saving best model based on accuracy
Epoch 4
Best Validation Accuracy: 0.7050065441956768
Current Validation Accuracy: 0.7062165794211467
Current Validation Loss: 0.00039985384975508746
Saving best model based on accuracy
Epoch 5
Best Validation Accuracy: 0.7062165794211467
Current Validation Accuracy: 0.7067212297709938
Current Validation Loss: 0.00039895057172946615
Saving best model based on accuracy
Epoch 6
Best Validation Accuracy: 0.7067212297709938
Current Validation Accuracy: 0.708217671266885
Current Validation Loss: 0.000397147427521538
Saving best model based on accuracy
Epoch 7
Best Validation Accuracy: 0.708217671266885
Current Validation Accuracy: 0.7093995661382742
Current Validation Loss: 0.0003956946946729546
Saving best model based on accuracy
Epoch 8
Best Validation Accuracy: 0.7093995661382742
Current Validation Accuracy: 0.7417972431207778
Current Validation Loss: 0.0003556448380471493
Saving best model based on accuracy
Epoch 9
Best Validation Accuracy: 0.7417972431207778
Current Validation Accuracy: 0.716731066388098
Current Validation Loss: 0.0003880007982031731
Epoch 10
Best Validation Accuracy: 0.7417972431207778
Current Validation Accuracy: 0.7069651128396932
Current Validation Loss: 0.00039963304445927447
Epoch 11
Best Validation Accuracy: 0.7417972431207778
Current Validation Accuracy: 0.7239431264683793
Current Validation Loss: 0.00037916360923761637
Epoch 12
Best Validation Accuracy: 0.7417972431207778
Current Validation Accuracy: 0.7385936201439911
Current Validation Loss: 0.0003597287850861493
Epoch 13
Best Validation Accuracy: 0.7417972431207778
Current Validation Accuracy: 0.729841344683693
Current Validation Loss: 0.000371263942565286
Epoch 14
Best Validation Accuracy: 0.7417972431207778
Current Validation Accuracy: 0.7347921709782901
Current Validation Loss: 0.0003644596054068856
Epoch 15
Best Validation Accuracy: 0.7417972431207778
Current Validation Accuracy: 0.760411149333355
Current Validation Loss: 0.00033000762555171384
Saving best model based on accuracy
Epoch 16
Best Validation Accuracy: 0.760411149333355
Current Validation Accuracy: 0.7640506351277916
Current Validation Loss: 0.00032530908053966914
Saving best model based on accuracy
Epoch 17
Best Validation Accuracy: 0.7640506351277916
Current Validation Accuracy: 0.7601797730886916
Current Validation Loss: 0.00033053816558438617
Epoch 18
Best Validation Accuracy: 0.7640506351277916
Current Validation Accuracy: 0.7649999030721137
Current Validation Loss: 0.0003238073751262566
Saving best model based on accuracy
Epoch 19
Best Validation Accuracy: 0.7649999030721137
Current Validation Accuracy: 0.7657009105593239
Current Validation Loss: 0.00032300250378482687
Saving best model based on accuracy
Epoch 20
Best Validation Accuracy: 0.7657009105593239
Current Validation Accuracy: 0.7568623380131785
Current Validation Loss: 0.00033516341858421035
Epoch 21
Best Validation Accuracy: 0.7657009105593239
Current Validation Accuracy: 0.7602623181273283
Current Validation Loss: 0.0003303707782836675
Epoch 22
Best Validation Accuracy: 0.7657009105593239
Current Validation Accuracy: 0.767539413692596
Current Validation Loss: 0.0003202712901128454
Saving best model based on accuracy
Epoch 23
Best Validation Accuracy: 0.767539413692596
Current Validation Accuracy: 0.7666720654457088
Current Validation Loss: 0.0003216505499503538
Epoch 24
Best Validation Accuracy: 0.767539413692596
Current Validation Accuracy: 0.7669234526088297
Current Validation Loss: 0.000321187524167666
Epoch 25
Best Validation Accuracy: 0.767539413692596
Current Validation Accuracy: 0.768601243053241
Current Validation Loss: 0.00031910158009626136
Saving best model based on accuracy
Epoch 26
Best Validation Accuracy: 0.768601243053241
Current Validation Accuracy: 0.7708405898968624
Current Validation Loss: 0.0003159477491949339
Saving best model based on accuracy
Epoch 27
Best Validation Accuracy: 0.7708405898968624
Current Validation Accuracy: 0.7708656035449342
Current Validation Loss: 0.0003157884436272476
Validation Accuracy has not changed much, will stop in 4 epochs if this continues
Saving best model based on accuracy
Epoch 28
Best Validation Accuracy: 0.7708656035449342
Current Validation Accuracy: 0.7702458904139571
Current Validation Loss: 0.0003167024324538985
Epoch 29
[[9.11644816e-01 9.81847756e-04 1.09831039e-02 6.40790537e-03
  1.14675437e-04 4.08805819e-04 2.76911240e-02 4.17678356e-02]
 [4.03609984e-02 2.11980846e-03 2.62978673e-01 6.24777019e-01
  5.88970967e-02 9.73049555e-06 8.68096016e-03 2.17577606e-03]
 [7.52102397e-03 8.05417355e-03 5.79594187e-02 3.92830931e-03
  8.77031498e-03 8.51558447e-01 5.10146655e-02 1.11935837e-02]
 [1.63122565e-01 1.42736826e-04 3.44261946e-03 3.79048213e-02
  1.56878878e-03 2.21037699e-04 3.64435583e-01 4.29161847e-01]
 [2.42093932e-02 1.74390122e-01 6.45545879e-06 1.18288200e-03
  8.56791402e-08 1.12859425e-07 8.00144911e-01 6.59820289e-05]
 [4.01186349e-04 1.82120002e-05 2.03876174e-03 4.51131957e-03
  9.92342293e-01 2.26986012e-05 2.76377395e-04 3.89147899e-04]
 [9.96505499e-01 1.21910443e-06 3.40496154e-05 1.76149661e-05
  5.52445329e-07 4.65724770e-06 1.64648972e-03 1.78983342e-03]
 [1.11011374e-04 2.01886884e-04 2.94012967e-02 9.98397078e-03
  9.59437966e-01 6.40881183e-07 2.13823165e-04 6.49356225e-04]
 [3.75416037e-03 1.36253933e-04 1.81295129e-03 1.21357990e-03
  4.34556557e-03 7.79758440e-04 5.32961726e-01 4.54995930e-01]
 [9.88878071e-01 2.03685995e-05 2.41107773e-04 1.57198709e-04
  3.37236611e-06 2.11491715e-05 5.12663648e-03 5.55209070e-03]
 [2.54131295e-02 7.21734134e-04 8.74701664e-02 1.45281672e-01
  6.94605470e-01 4.90349012e-05 2.33071875e-02 2.31516045e-02]
 [3.08808289e-03 5.50904989e-01 2.74449517e-03 8.20439775e-03
  2.67216354e-04 2.35069783e-05 4.34599549e-01 1.67819613e-04]
 [9.73261952e-01 1.00291190e-05 2.08270526e-03 3.23694476e-05
  1.04069397e-06 1.38212476e-04 8.32305476e-03 1.61506273e-02]
 [1.56942606e-02 4.03195183e-04 2.53666285e-02 1.37043059e-01
  8.05955231e-01 4.07715925e-05 7.74139538e-03 7.75545416e-03]
 [1.33500835e-05 3.92336415e-06 3.24064604e-05 1.31274392e-05
  6.02422442e-05 9.99852777e-01 5.50148752e-06 1.86791276e-05]
 [1.44359812e-01 2.60862143e-04 4.91799507e-03 8.89703557e-02
  6.11238062e-01 4.99578018e-04 1.09810382e-01 3.99429537e-02]
 [2.01630456e-07 5.58832660e-08 3.09263122e-08 1.77954540e-09
  1.76069545e-06 9.99997973e-01 1.72813106e-08 6.27191077e-10]
 [2.40970757e-02 3.99669213e-03 4.66021672e-02 2.55961772e-02
  4.28992301e-01 3.61289218e-04 3.33626300e-01 1.36727959e-01]
 [9.54064131e-02 3.24485067e-04 2.11109459e-01 1.81673124e-01
  4.39049006e-01 1.34993446e-04 5.03272526e-02 2.19752602e-02]
 [1.25694573e-01 1.93693936e-01 2.04548310e-03 6.66861248e-04
  9.17008583e-06 1.99034639e-05 6.66905522e-01 1.09644914e-02]]
[[9.11644816e-01 9.81847756e-04 1.09831039e-02 6.40790537e-03
  1.14675437e-04 4.08805819e-04 2.76911240e-02 4.17678356e-02]
 [4.03609984e-02 2.11980846e-03 2.62978673e-01 6.24777019e-01
  5.88970967e-02 9.73049555e-06 8.68096016e-03 2.17577606e-03]
 [7.52102397e-03 8.05417355e-03 5.79594187e-02 3.92830931e-03
  8.77031498e-03 8.51558447e-01 5.10146655e-02 1.11935837e-02]
 [1.63122565e-01 1.42736826e-04 3.44261946e-03 3.79048213e-02
  1.56878878e-03 2.21037699e-04 3.64435583e-01 4.29161847e-01]
 [2.42093932e-02 1.74390122e-01 6.45545879e-06 1.18288200e-03
  8.56791402e-08 1.12859425e-07 8.00144911e-01 6.59820289e-05]
 [4.01186349e-04 1.82120002e-05 2.03876174e-03 4.51131957e-03
  9.92342293e-01 2.26986012e-05 2.76377395e-04 3.89147899e-04]
 [9.96505499e-01 1.21910443e-06 3.40496154e-05 1.76149661e-05
  5.52445329e-07 4.65724770e-06 1.64648972e-03 1.78983342e-03]
 [1.11011374e-04 2.01886884e-04 2.94012967e-02 9.98397078e-03
  9.59437966e-01 6.40881183e-07 2.13823165e-04 6.49356225e-04]
 [3.75416037e-03 1.36253933e-04 1.81295129e-03 1.21357990e-03
  4.34556557e-03 7.79758440e-04 5.32961726e-01 4.54995930e-01]
 [9.88878071e-01 2.03685995e-05 2.41107773e-04 1.57198709e-04
  3.37236611e-06 2.11491715e-05 5.12663648e-03 5.55209070e-03]
 [2.54131295e-02 7.21734134e-04 8.74701664e-02 1.45281672e-01
  6.94605470e-01 4.90349012e-05 2.33071875e-02 2.31516045e-02]
 [3.08808289e-03 5.50904989e-01 2.74449517e-03 8.20439775e-03
  2.67216354e-04 2.35069783e-05 4.34599549e-01 1.67819613e-04]
 [9.73261952e-01 1.00291190e-05 2.08270526e-03 3.23694476e-05
  1.04069397e-06 1.38212476e-04 8.32305476e-03 1.61506273e-02]
 [1.56942606e-02 4.03195183e-04 2.53666285e-02 1.37043059e-01
  8.05955231e-01 4.07715925e-05 7.74139538e-03 7.75545416e-03]
 [1.33500835e-05 3.92336415e-06 3.24064604e-05 1.31274392e-05
  6.02422442e-05 9.99852777e-01 5.50148752e-06 1.86791276e-05]
 [1.44359812e-01 2.60862143e-04 4.91799507e-03 8.89703557e-02
  6.11238062e-01 4.99578018e-04 1.09810382e-01 3.99429537e-02]
 [2.01630456e-07 5.58832660e-08 3.09263122e-08 1.77954540e-09
  1.76069545e-06 9.99997973e-01 1.72813106e-08 6.27191077e-10]
 [2.40970757e-02 3.99669213e-03 4.66021672e-02 2.55961772e-02
  4.28992301e-01 3.61289218e-04 3.33626300e-01 1.36727959e-01]
 [9.54064131e-02 3.24485067e-04 2.11109459e-01 1.81673124e-01
  4.39049006e-01 1.34993446e-04 5.03272526e-02 2.19752602e-02]
 [1.25694573e-01 1.93693936e-01 2.04548310e-03 6.66861248e-04
  9.17008583e-06 1.99034639e-05 6.66905522e-01 1.09644914e-02]]
[[1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]]
Best Validation Accuracy: 0.7708656035449342
Current Validation Accuracy: 0.7707505407638042
Current Validation Loss: 0.00031584732356841383
Saving last model
