Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
A conda environment has been detected CONDA_PREFIX=
/sw/external/python/anaconda3 
anaconda3_gpu is loaded. Consider running conda deactivate and reloading it.

Currently Loaded Modules:
  1) cue-login-env/1.0   4) gcc/11.2.0    7) openmpi/4.1.2
  2) default             5) cuda/11.6.1   8) cudnn/8.4.1.50
  3) modtree/gpu         6) ucx/1.11.2    9) anaconda3_gpu/4.13.0

 

/u/avroy/UQ_XAI_Studies/models/PFIN_UQ/model/jobsub
total 508K
-rw-rw----+ 1 avroy delta_bbhj  925 Mar 31 16:58 jobsubmitter_dummy.py
-rw-rw----+ 1 avroy delta_bbhj 5.2K May 27 21:16 jobsubmitter_jetclass_1gpu.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 27 21:04 jobsubmitter_jetclass_1gpu.py~
-rw-rw----+ 1 avroy delta_bbhj  743 May 19 01:04 jobsubmitter_jetclass.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 18 13:59 jobsubmitter.py
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 19 13:52 UQPFIN-jetclass-run-jetclass20M_0_baseline.slurm
-rw-------+ 1 avroy delta_bbhj  973 May 19 00:59 UQPFIN-jetclass-run.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_9.slurm
-rw-------+ 1 avroy delta_bbhj  969 May 15 09:44 UQPFIN-run.slurm
/projects/bbhj/avroy/UQ_XAI_Studies/models/PFIN_UQ/model
total 184K
drwxr-s---+ 2 avroy delta_bbhj  12K May 28 01:44 jobouts
drwxr-s---+ 2 avroy delta_bbhj  16K May 27 21:18 jobsub
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 18 21:37 PFINDataset.py
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 15 14:46 PFINDataset.py~
drwxrws---+ 2 avroy delta_bbhj 4.0K May 18 21:38 __pycache__
drwxrws---+ 2 avroy delta_bbhj  16K May 27 23:34 trained_model_dicts
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:09 trained_model_dicts_old
drwxrws---+ 2 avroy delta_bbhj  24K May 27 23:53 trained_models
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:10 trained_models_old
-rw-rw----+ 1 avroy delta_bbhj  23K May 18 15:52 train_mod.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 27 21:06 train.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 18 13:50 train.py~
-rw-rw----+ 1 avroy delta_bbhj  13K May  5 09:36 UQPFIN.py
job is starting on gpua008.delta.ncsa.illinois.edu
# conda environments:
#
base                     /sw/external/python/anaconda3
toptagger_env         *  /u/avroy/.conda/envs/toptagger_env

/sw/external/python/anaconda3_gpu/bin/python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UQPFIN                                   [1, 10]                   --
├─Sequential: 1-1                        [60, 64]                  --
│    └─Sequential: 2-1                   [60, 100]                 --
│    │    └─Linear: 3-1                  [60, 100]                 1,200
│    │    └─ReLU: 3-2                    [60, 100]                 --
│    └─Sequential: 2-2                   [60, 100]                 --
│    │    └─Linear: 3-3                  [60, 100]                 10,100
│    │    └─ReLU: 3-4                    [60, 100]                 --
│    └─Sequential: 2-3                   [60, 64]                  --
│    │    └─Linear: 3-5                  [60, 64]                  6,464
│    │    └─ReLU: 3-6                    [60, 64]                  --
├─Sequential: 1-2                        [1770, 64]                --
│    └─Sequential: 2-4                   [1770, 128]               --
│    │    └─Linear: 3-7                  [1770, 128]               640
│    │    └─ReLU: 3-8                    [1770, 128]               --
│    └─Sequential: 2-5                   [1770, 128]               --
│    │    └─Linear: 3-9                  [1770, 128]               16,512
│    │    └─ReLU: 3-10                   [1770, 128]               --
│    └─Sequential: 2-6                   [1770, 64]                --
│    │    └─Linear: 3-11                 [1770, 64]                8,256
│    │    └─ReLU: 3-12                   [1770, 64]                --
├─Sequential: 1-3                        [60, 64]                  --
│    └─Sequential: 2-7                   [60, 128]                 --
│    │    └─Linear: 3-13                 [60, 128]                 9,728
│    │    └─ReLU: 3-14                   [60, 128]                 --
│    └─Sequential: 2-8                   [60, 128]                 --
│    │    └─Linear: 3-15                 [60, 128]                 16,512
│    │    └─ReLU: 3-16                   [60, 128]                 --
│    └─Sequential: 2-9                   [60, 64]                  --
│    │    └─Linear: 3-17                 [60, 64]                  8,256
│    │    └─ReLU: 3-18                   [60, 64]                  --
├─Sequential: 1-4                        [1, 10]                   --
│    └─Sequential: 2-10                  [1, 64]                   --
│    │    └─Linear: 3-19                 [1, 64]                   4,160
│    │    └─ReLU: 3-20                   [1, 64]                   --
│    └─Sequential: 2-11                  [1, 100]                  --
│    │    └─Linear: 3-21                 [1, 100]                  6,500
│    │    └─ReLU: 3-22                   [1, 100]                  --
│    └─Sequential: 2-12                  [1, 100]                  --
│    │    └─Linear: 3-23                 [1, 100]                  10,100
│    │    └─ReLU: 3-24                   [1, 100]                  --
│    └─Linear: 2-13                      [1, 10]                   1,010
│    └─Softmax: 2-14                     [1, 10]                   --
==========================================================================================
Total params: 99,438
Trainable params: 99,438
Non-trainable params: 0
Total mult-adds (M): 48.13
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 4.81
Params size (MB): 0.40
Estimated Total Size (MB): 5.21
==========================================================================================
data type:  jetclass
m_logic, pt_logic, eta_logic:  AND AND AND
m1, m2, pt1, pt2, eta1, eta2:  0.0 10000.0 0.0 10000.0 -6.0 6.0
skip labels:  []
classes:  10
Epoch 0
Best Validation Accuracy: 0
Current Validation Accuracy: 0.7103249497651204
Current Validation Loss: 0.000324285965996572
Saving best model based on accuracy
Epoch 1
Best Validation Accuracy: 0.7103249497651204
Current Validation Accuracy: 0.7218440815383254
Current Validation Loss: 0.00031111862980331083
Saving best model based on accuracy
Epoch 2
Best Validation Accuracy: 0.7218440815383254
Current Validation Accuracy: 0.7242261427465936
Current Validation Loss: 0.00030755523065322964
Saving best model based on accuracy
Epoch 3
Best Validation Accuracy: 0.7242261427465936
Current Validation Accuracy: 0.726878824516322
Current Validation Loss: 0.0003043268123623444
Saving best model based on accuracy
Epoch 4
Best Validation Accuracy: 0.726878824516322
Current Validation Accuracy: 0.7309681463091807
Current Validation Loss: 0.0003002940808804238
Saving best model based on accuracy
Epoch 5
Best Validation Accuracy: 0.7309681463091807
Current Validation Accuracy: 0.7345862581780183
Current Validation Loss: 0.0002971119638690432
Saving best model based on accuracy
Epoch 6
Best Validation Accuracy: 0.7345862581780183
Current Validation Accuracy: 0.7356592361897225
Current Validation Loss: 0.00029578449375462486
Saving best model based on accuracy
Epoch 7
Best Validation Accuracy: 0.7356592361897225
Current Validation Accuracy: 0.750092666282829
Current Validation Loss: 0.0002817110878241983
Saving best model based on accuracy
Epoch 8
Best Validation Accuracy: 0.750092666282829
Current Validation Accuracy: 0.75765103353544
Current Validation Loss: 0.0002737328342832438
Saving best model based on accuracy
Epoch 9
Best Validation Accuracy: 0.75765103353544
Current Validation Accuracy: 0.7426013288920214
Current Validation Loss: 0.00028965530717974317
Epoch 10
Best Validation Accuracy: 0.75765103353544
Current Validation Accuracy: 0.7736296520099705
Current Validation Loss: 0.00025505467682175993
Saving best model based on accuracy
Epoch 11
Best Validation Accuracy: 0.7736296520099705
Current Validation Accuracy: 0.7766284879914002
Current Validation Loss: 0.0002518195817733153
Saving best model based on accuracy
Epoch 12
Best Validation Accuracy: 0.7766284879914002
Current Validation Accuracy: 0.7683543018414704
Current Validation Loss: 0.0002610285854240421
Epoch 13
Best Validation Accuracy: 0.7766284879914002
Current Validation Accuracy: 0.766068783643113
Current Validation Loss: 0.0002639735889151781
Epoch 14
Best Validation Accuracy: 0.7766284879914002
Current Validation Accuracy: 0.7759356793451483
Current Validation Loss: 0.0002521572923704334
Epoch 15
Best Validation Accuracy: 0.7766284879914002
Current Validation Accuracy: 0.7858205830697576
Current Validation Loss: 0.0002420036943582733
Saving best model based on accuracy
Epoch 16
Best Validation Accuracy: 0.7858205830697576
Current Validation Accuracy: 0.7852318207761558
Current Validation Loss: 0.00024234994661942726
Epoch 17
Best Validation Accuracy: 0.7858205830697576
Current Validation Accuracy: 0.7904026243691564
Current Validation Loss: 0.00023641399009234566
Saving best model based on accuracy
Epoch 18
Best Validation Accuracy: 0.7904026243691564
Current Validation Accuracy: 0.7902750675425902
Current Validation Loss: 0.00023617347845775627
Epoch 19
Best Validation Accuracy: 0.7904026243691564
Current Validation Accuracy: 0.7901410078189833
Current Validation Loss: 0.00023661709450210468
Epoch 20
Best Validation Accuracy: 0.7904026243691564
Current Validation Accuracy: 0.7915251244429393
Current Validation Loss: 0.00023511721386145068
Saving best model based on accuracy
Epoch 21
Best Validation Accuracy: 0.7915251244429393
Current Validation Accuracy: 0.7916441774810679
Current Validation Loss: 0.00023506076609295808
Saving best model based on accuracy
Epoch 22
Best Validation Accuracy: 0.7916441774810679
Current Validation Accuracy: 0.7928006927086017
Current Validation Loss: 0.00023382451220135215
Saving best model based on accuracy
Epoch 23
Best Validation Accuracy: 0.7928006927086017
Current Validation Accuracy: 0.7918197557011649
Current Validation Loss: 0.0002348302829670067
Epoch 24
Best Validation Accuracy: 0.7928006927086017
Current Validation Accuracy: 0.7930953239668272
Current Validation Loss: 0.00023321765231933668
Saving best model based on accuracy
Epoch 25
Best Validation Accuracy: 0.7930953239668272
Current Validation Accuracy: 0.7934294728301459
Current Validation Loss: 0.00023257247026101284
Saving best model based on accuracy
Epoch 26
Best Validation Accuracy: 0.7934294728301459
Current Validation Accuracy: 0.7941627995271894
Current Validation Loss: 0.00023163123239133937
Saving best model based on accuracy
Epoch 27
Best Validation Accuracy: 0.7941627995271894
Current Validation Accuracy: 0.7938651669318681
Current Validation Loss: 0.000231929086381841
Epoch 28
Best Validation Accuracy: 0.7941627995271894
Current Validation Accuracy: 0.7955008956490116
Current Validation Loss: 0.00023013543583265994
Saving best model based on accuracy
Epoch 29
[[9.61413741e-01 4.26142040e-04 3.53464321e-03 3.32072587e-03
  5.15542924e-05 1.25678838e-04 1.59996487e-02 1.50098037e-02
  5.16762429e-06 1.12884263e-04]
 [2.53912807e-02 1.06582348e-03 2.43253186e-01 6.36642098e-01
  8.33580866e-02 9.32813236e-06 6.68369839e-03 1.44329190e-03
  2.15017889e-03 3.07051687e-06]
 [8.37944810e-07 4.36485578e-07 2.25622193e-06 9.08312270e-09
  9.19187004e-10 1.23372430e-03 4.70841961e-08 2.03060058e-08
  7.09012182e-10 9.98762727e-01]
 [4.84930212e-03 3.26464209e-03 5.62728532e-02 3.29892826e-03
  1.12641640e-02 8.28203619e-01 6.64137155e-02 2.46242397e-02
  1.11666135e-03 6.91866910e-04]
 [2.72180159e-07 8.67346728e-07 2.16498108e-08 3.12927151e-09
  4.65676830e-10 5.27767173e-04 6.75988119e-08 3.10645930e-07
  2.62893877e-07 9.99470413e-01]
 [1.17108285e-01 1.65538775e-04 6.01431262e-03 3.29108797e-02
  8.36617895e-04 6.64027582e-04 4.71728384e-01 3.70535910e-01
  3.33620737e-05 2.61080299e-06]
 [2.44423281e-02 2.10753515e-01 3.20567851e-05 2.75077042e-03
  5.21516040e-06 8.39203551e-07 7.59604096e-01 2.43446790e-04
  1.63994124e-03 5.27835975e-04]
 [2.12588173e-04 4.45393380e-06 1.91754429e-03 3.00916960e-03
  9.94505942e-01 2.19597769e-05 9.12398391e-05 1.51968328e-04
  8.51550940e-05 2.51357051e-08]
 [9.96414125e-01 3.46782099e-06 5.15513930e-05 1.63574805e-05
  2.36268306e-07 1.23050731e-05 2.31770123e-03 1.16914592e-03
  1.33030582e-08 1.51272016e-05]
 [1.55470771e-04 9.45055799e-05 3.66809778e-02 6.01396943e-03
  9.55637932e-01 4.66063466e-06 1.51712651e-04 3.49430629e-04
  9.11330048e-04 4.15048369e-08]
 [4.42175940e-03 3.29741306e-05 1.90043438e-03 7.75862020e-04
  2.46304157e-03 1.56329502e-03 3.56806010e-01 6.32003367e-01
  3.14824792e-05 1.75498712e-06]
 [9.87066090e-01 3.17334052e-05 4.00891062e-04 2.90888915e-04
  1.90735545e-06 1.51625754e-05 6.86585251e-03 5.31166559e-03
  3.41992859e-07 1.54122390e-05]
 [1.25734853e-02 1.11086702e-03 9.16980579e-02 1.56646624e-01
  7.05770493e-01 4.83099939e-05 7.34570250e-03 7.47994054e-03
  1.73243694e-02 2.14531246e-06]
 [1.39762484e-03 7.65912294e-01 4.84467018e-03 6.59971870e-03
  3.15370446e-04 3.07496957e-05 2.20420197e-01 1.03511564e-04
  3.68421257e-04 7.35882168e-06]
 [9.81880546e-01 9.74817794e-06 1.80924463e-03 4.81265342e-05
  3.77746176e-07 5.23858034e-06 9.93202534e-03 5.85906859e-03
  4.42786695e-04 1.29246255e-05]
 [1.74568258e-02 3.11765005e-04 3.77617218e-02 1.13960408e-01
  7.94385016e-01 7.35975555e-05 7.64277950e-03 5.53762540e-03
  2.28654053e-02 4.85123519e-06]
 [2.68582735e-05 2.19911749e-06 6.65616462e-05 2.61503719e-05
  4.35644833e-05 9.99286950e-01 5.35823710e-05 1.33354493e-04
  1.53078724e-06 3.59179481e-04]
 [9.48119909e-02 7.61507981e-05 2.32036063e-03 3.93447317e-02
  3.47851634e-01 4.07019164e-04 3.10531911e-02 2.29126960e-02
  4.61210281e-01 1.20359091e-05]
 [1.03585201e-03 3.87794338e-03 6.31731004e-03 1.01187243e-03
  1.82320899e-03 1.11007968e-04 2.61969608e-03 2.66705267e-03
  9.80484188e-01 5.18681736e-05]
 [8.30497868e-07 2.54917921e-07 5.96838936e-06 3.46267193e-08
  9.08757215e-07 9.77166951e-01 2.41657199e-06 3.06684058e-07
  1.75362569e-08 2.28222460e-02]]
[[9.61413741e-01 4.26142040e-04 3.53464321e-03 3.32072587e-03
  5.15542924e-05 1.25678838e-04 1.59996487e-02 1.50098037e-02
  5.16762429e-06 1.12884263e-04]
 [2.53912807e-02 1.06582348e-03 2.43253186e-01 6.36642098e-01
  8.33580866e-02 9.32813236e-06 6.68369839e-03 1.44329190e-03
  2.15017889e-03 3.07051687e-06]
 [8.37944810e-07 4.36485578e-07 2.25622193e-06 9.08312270e-09
  9.19187004e-10 1.23372430e-03 4.70841961e-08 2.03060058e-08
  7.09012182e-10 9.98762727e-01]
 [4.84930212e-03 3.26464209e-03 5.62728532e-02 3.29892826e-03
  1.12641640e-02 8.28203619e-01 6.64137155e-02 2.46242397e-02
  1.11666135e-03 6.91866910e-04]
 [2.72180159e-07 8.67346728e-07 2.16498108e-08 3.12927151e-09
  4.65676830e-10 5.27767173e-04 6.75988119e-08 3.10645930e-07
  2.62893877e-07 9.99470413e-01]
 [1.17108285e-01 1.65538775e-04 6.01431262e-03 3.29108797e-02
  8.36617895e-04 6.64027582e-04 4.71728384e-01 3.70535910e-01
  3.33620737e-05 2.61080299e-06]
 [2.44423281e-02 2.10753515e-01 3.20567851e-05 2.75077042e-03
  5.21516040e-06 8.39203551e-07 7.59604096e-01 2.43446790e-04
  1.63994124e-03 5.27835975e-04]
 [2.12588173e-04 4.45393380e-06 1.91754429e-03 3.00916960e-03
  9.94505942e-01 2.19597769e-05 9.12398391e-05 1.51968328e-04
  8.51550940e-05 2.51357051e-08]
 [9.96414125e-01 3.46782099e-06 5.15513930e-05 1.63574805e-05
  2.36268306e-07 1.23050731e-05 2.31770123e-03 1.16914592e-03
  1.33030582e-08 1.51272016e-05]
 [1.55470771e-04 9.45055799e-05 3.66809778e-02 6.01396943e-03
  9.55637932e-01 4.66063466e-06 1.51712651e-04 3.49430629e-04
  9.11330048e-04 4.15048369e-08]
 [4.42175940e-03 3.29741306e-05 1.90043438e-03 7.75862020e-04
  2.46304157e-03 1.56329502e-03 3.56806010e-01 6.32003367e-01
  3.14824792e-05 1.75498712e-06]
 [9.87066090e-01 3.17334052e-05 4.00891062e-04 2.90888915e-04
  1.90735545e-06 1.51625754e-05 6.86585251e-03 5.31166559e-03
  3.41992859e-07 1.54122390e-05]
 [1.25734853e-02 1.11086702e-03 9.16980579e-02 1.56646624e-01
  7.05770493e-01 4.83099939e-05 7.34570250e-03 7.47994054e-03
  1.73243694e-02 2.14531246e-06]
 [1.39762484e-03 7.65912294e-01 4.84467018e-03 6.59971870e-03
  3.15370446e-04 3.07496957e-05 2.20420197e-01 1.03511564e-04
  3.68421257e-04 7.35882168e-06]
 [9.81880546e-01 9.74817794e-06 1.80924463e-03 4.81265342e-05
  3.77746176e-07 5.23858034e-06 9.93202534e-03 5.85906859e-03
  4.42786695e-04 1.29246255e-05]
 [1.74568258e-02 3.11765005e-04 3.77617218e-02 1.13960408e-01
  7.94385016e-01 7.35975555e-05 7.64277950e-03 5.53762540e-03
  2.28654053e-02 4.85123519e-06]
 [2.68582735e-05 2.19911749e-06 6.65616462e-05 2.61503719e-05
  4.35644833e-05 9.99286950e-01 5.35823710e-05 1.33354493e-04
  1.53078724e-06 3.59179481e-04]
 [9.48119909e-02 7.61507981e-05 2.32036063e-03 3.93447317e-02
  3.47851634e-01 4.07019164e-04 3.10531911e-02 2.29126960e-02
  4.61210281e-01 1.20359091e-05]
 [1.03585201e-03 3.87794338e-03 6.31731004e-03 1.01187243e-03
  1.82320899e-03 1.11007968e-04 2.61969608e-03 2.66705267e-03
  9.80484188e-01 5.18681736e-05]
 [8.30497868e-07 2.54917921e-07 5.96838936e-06 3.46267193e-08
  9.08757215e-07 9.77166951e-01 2.41657199e-06 3.06684058e-07
  1.75362569e-08 2.28222460e-02]]
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
Best Validation Accuracy: 0.7955008956490116
Current Validation Accuracy: 0.7960296312006999
Current Validation Loss: 0.0002294819923173582
Saving best model based on accuracy
Saving last model
