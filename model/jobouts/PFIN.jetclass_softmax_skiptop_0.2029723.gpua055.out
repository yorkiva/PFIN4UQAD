Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
A conda environment has been detected CONDA_PREFIX=
/sw/external/python/anaconda3 
anaconda3_gpu is loaded. Consider running conda deactivate and reloading it.

Currently Loaded Modules:
  1) cue-login-env/1.0   4) gcc/11.2.0    7) openmpi/4.1.2
  2) default             5) cuda/11.6.1   8) cudnn/8.4.1.50
  3) modtree/gpu         6) ucx/1.11.2    9) anaconda3_gpu/4.13.0

 

/u/avroy/UQ_XAI_Studies/models/PFIN_UQ/model/jobsub
total 508K
-rw-rw----+ 1 avroy delta_bbhj  925 Mar 31 16:58 jobsubmitter_dummy.py
-rw-rw----+ 1 avroy delta_bbhj 5.2K May 27 21:16 jobsubmitter_jetclass_1gpu.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 27 21:04 jobsubmitter_jetclass_1gpu.py~
-rw-rw----+ 1 avroy delta_bbhj  743 May 19 01:04 jobsubmitter_jetclass.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 18 13:59 jobsubmitter.py
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 19 13:52 UQPFIN-jetclass-run-jetclass20M_0_baseline.slurm
-rw-------+ 1 avroy delta_bbhj  973 May 19 00:59 UQPFIN-jetclass-run.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_9.slurm
-rw-------+ 1 avroy delta_bbhj  969 May 15 09:44 UQPFIN-run.slurm
/projects/bbhj/avroy/UQ_XAI_Studies/models/PFIN_UQ/model
total 184K
drwxr-s---+ 2 avroy delta_bbhj  12K May 28 01:44 jobouts
drwxr-s---+ 2 avroy delta_bbhj  16K May 27 21:18 jobsub
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 18 21:37 PFINDataset.py
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 15 14:46 PFINDataset.py~
drwxrws---+ 2 avroy delta_bbhj 4.0K May 18 21:38 __pycache__
drwxrws---+ 2 avroy delta_bbhj  16K May 27 23:34 trained_model_dicts
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:09 trained_model_dicts_old
drwxrws---+ 2 avroy delta_bbhj  24K May 27 23:53 trained_models
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:10 trained_models_old
-rw-rw----+ 1 avroy delta_bbhj  23K May 18 15:52 train_mod.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 27 21:06 train.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 18 13:50 train.py~
-rw-rw----+ 1 avroy delta_bbhj  13K May  5 09:36 UQPFIN.py
job is starting on gpua055.delta.ncsa.illinois.edu
# conda environments:
#
base                     /sw/external/python/anaconda3
toptagger_env         *  /u/avroy/.conda/envs/toptagger_env

/sw/external/python/anaconda3_gpu/bin/python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UQPFIN                                   [1, 8]                    --
├─Sequential: 1-1                        [60, 64]                  --
│    └─Sequential: 2-1                   [60, 100]                 --
│    │    └─Linear: 3-1                  [60, 100]                 1,200
│    │    └─ReLU: 3-2                    [60, 100]                 --
│    └─Sequential: 2-2                   [60, 100]                 --
│    │    └─Linear: 3-3                  [60, 100]                 10,100
│    │    └─ReLU: 3-4                    [60, 100]                 --
│    └─Sequential: 2-3                   [60, 64]                  --
│    │    └─Linear: 3-5                  [60, 64]                  6,464
│    │    └─ReLU: 3-6                    [60, 64]                  --
├─Sequential: 1-2                        [1770, 64]                --
│    └─Sequential: 2-4                   [1770, 128]               --
│    │    └─Linear: 3-7                  [1770, 128]               640
│    │    └─ReLU: 3-8                    [1770, 128]               --
│    └─Sequential: 2-5                   [1770, 128]               --
│    │    └─Linear: 3-9                  [1770, 128]               16,512
│    │    └─ReLU: 3-10                   [1770, 128]               --
│    └─Sequential: 2-6                   [1770, 64]                --
│    │    └─Linear: 3-11                 [1770, 64]                8,256
│    │    └─ReLU: 3-12                   [1770, 64]                --
├─Sequential: 1-3                        [60, 64]                  --
│    └─Sequential: 2-7                   [60, 128]                 --
│    │    └─Linear: 3-13                 [60, 128]                 9,728
│    │    └─ReLU: 3-14                   [60, 128]                 --
│    └─Sequential: 2-8                   [60, 128]                 --
│    │    └─Linear: 3-15                 [60, 128]                 16,512
│    │    └─ReLU: 3-16                   [60, 128]                 --
│    └─Sequential: 2-9                   [60, 64]                  --
│    │    └─Linear: 3-17                 [60, 64]                  8,256
│    │    └─ReLU: 3-18                   [60, 64]                  --
├─Sequential: 1-4                        [1, 8]                    --
│    └─Sequential: 2-10                  [1, 64]                   --
│    │    └─Linear: 3-19                 [1, 64]                   4,160
│    │    └─ReLU: 3-20                   [1, 64]                   --
│    └─Sequential: 2-11                  [1, 100]                  --
│    │    └─Linear: 3-21                 [1, 100]                  6,500
│    │    └─ReLU: 3-22                   [1, 100]                  --
│    └─Sequential: 2-12                  [1, 100]                  --
│    │    └─Linear: 3-23                 [1, 100]                  10,100
│    │    └─ReLU: 3-24                   [1, 100]                  --
│    └─Linear: 2-13                      [1, 8]                    808
│    └─Softmax: 2-14                     [1, 8]                    --
==========================================================================================
Total params: 99,236
Trainable params: 99,236
Non-trainable params: 0
Total mult-adds (M): 48.13
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 4.81
Params size (MB): 0.40
Estimated Total Size (MB): 5.21
==========================================================================================
data type:  jetclass
m_logic, pt_logic, eta_logic:  AND AND AND
m1, m2, pt1, pt2, eta1, eta2:  0.0 10000.0 0.0 10000.0 -6.0 6.0
skip labels:  [8, 9]
classes:  8
Epoch 0
Best Validation Accuracy: 0
Current Validation Accuracy: 0.676873694209403
Current Validation Loss: 0.00043280569507755484
Saving best model based on accuracy
Epoch 1
Best Validation Accuracy: 0.676873694209403
Current Validation Accuracy: 0.6956076659327245
Current Validation Loss: 0.00041230498660099453
Saving best model based on accuracy
Epoch 2
Best Validation Accuracy: 0.6956076659327245
Current Validation Accuracy: 0.7011882108175274
Current Validation Loss: 0.0004053601594811054
Saving best model based on accuracy
Epoch 3
Best Validation Accuracy: 0.7011882108175274
Current Validation Accuracy: 0.7050196763609144
Current Validation Loss: 0.00040103374349181143
Saving best model based on accuracy
Epoch 4
Best Validation Accuracy: 0.7050196763609144
Current Validation Accuracy: 0.7061202768760705
Current Validation Loss: 0.00039954270172877965
Saving best model based on accuracy
Epoch 5
Best Validation Accuracy: 0.7061202768760705
Current Validation Accuracy: 0.7069951292173793
Current Validation Loss: 0.00039833287016280324
Saving best model based on accuracy
Epoch 6
Best Validation Accuracy: 0.7069951292173793
Current Validation Accuracy: 0.7050409379617754
Current Validation Loss: 0.00040013377644811674
Epoch 7
Best Validation Accuracy: 0.7069951292173793
Current Validation Accuracy: 0.7214536431440405
Current Validation Loss: 0.0003820122736849494
Saving best model based on accuracy
Epoch 8
Best Validation Accuracy: 0.7214536431440405
Current Validation Accuracy: 0.739117030729892
Current Validation Loss: 0.0003591845148963521
Saving best model based on accuracy
Epoch 9
Best Validation Accuracy: 0.739117030729892
Current Validation Accuracy: 0.7492144151152472
Current Validation Loss: 0.0003455847832126125
Saving best model based on accuracy
Epoch 10
Best Validation Accuracy: 0.7492144151152472
Current Validation Accuracy: 0.7519546602615051
Current Validation Loss: 0.00034141851501714033
Saving best model based on accuracy
Epoch 11
Best Validation Accuracy: 0.7519546602615051
Current Validation Accuracy: 0.7564883839745061
Current Validation Loss: 0.0003352406305112826
Saving best model based on accuracy
Epoch 12
Best Validation Accuracy: 0.7564883839745061
Current Validation Accuracy: 0.7454348528916089
Current Validation Loss: 0.00034984411460056177
Epoch 13
Best Validation Accuracy: 0.7564883839745061
Current Validation Accuracy: 0.7305479802417194
Current Validation Loss: 0.0003699562484298784
Epoch 14
Best Validation Accuracy: 0.7564883839745061
Current Validation Accuracy: 0.7566140775560666
Current Validation Loss: 0.0003354047709400477
Saving best model based on accuracy
Epoch 15
Best Validation Accuracy: 0.7566140775560666
Current Validation Accuracy: 0.7576702788458953
Current Validation Loss: 0.0003338337938433755
Saving best model based on accuracy
Epoch 16
Best Validation Accuracy: 0.7576702788458953
Current Validation Accuracy: 0.766229949216041
Current Validation Loss: 0.00032212920939727013
Saving best model based on accuracy
Epoch 17
Best Validation Accuracy: 0.766229949216041
Current Validation Accuracy: 0.763142014361586
Current Validation Loss: 0.00032627569310593913
Epoch 18
Best Validation Accuracy: 0.766229949216041
Current Validation Accuracy: 0.7687844680253664
Current Validation Loss: 0.00031873177278636923
Saving best model based on accuracy
Epoch 19
Best Validation Accuracy: 0.7687844680253664
Current Validation Accuracy: 0.7667214674006505
Current Validation Loss: 0.00032148055825652295
Epoch 20
Best Validation Accuracy: 0.7687844680253664
Current Validation Accuracy: 0.7678164398449904
Current Validation Loss: 0.00032025045235882065
Epoch 21
Best Validation Accuracy: 0.7687844680253664
Current Validation Accuracy: 0.769402305132738
Current Validation Loss: 0.00031755626473090645
Saving best model based on accuracy
Epoch 22
Best Validation Accuracy: 0.769402305132738
Current Validation Accuracy: 0.7700464065705851
Current Validation Loss: 0.0003166204795197662
Saving best model based on accuracy
Epoch 23
Best Validation Accuracy: 0.7700464065705851
Current Validation Accuracy: 0.7702146233538675
Current Validation Loss: 0.00031638533778030327
Saving best model based on accuracy
Epoch 24
Best Validation Accuracy: 0.7702146233538675
Current Validation Accuracy: 0.7706498608303155
Current Validation Loss: 0.0003155978832921862
Saving best model based on accuracy
Epoch 25
Best Validation Accuracy: 0.7706498608303155
Current Validation Accuracy: 0.7709425205127548
Current Validation Loss: 0.0003152900055454268
Saving best model based on accuracy
Epoch 26
Best Validation Accuracy: 0.7709425205127548
Current Validation Accuracy: 0.77140464766088
Current Validation Loss: 0.0003148577788274793
Saving best model based on accuracy
Epoch 27
Best Validation Accuracy: 0.77140464766088
Current Validation Accuracy: 0.7720093526030141
Current Validation Loss: 0.0003137909377728567
Saving best model based on accuracy
Epoch 28
Best Validation Accuracy: 0.7720093526030141
Current Validation Accuracy: 0.7719637026952831
Current Validation Loss: 0.0003137381608029473
Validation Accuracy has not changed much, will stop in 4 epochs if this continues
Epoch 29
[[9.7414613e-01 5.9112109e-04 1.7689185e-03 2.2762164e-03 1.4040008e-05
  5.5915214e-05 1.0072201e-02 1.1075475e-02]
 [3.8914796e-02 1.2456113e-03 1.4001562e-01 7.4901158e-01 5.4608461e-02
  2.0241843e-05 1.3232293e-02 2.9514425e-03]
 [1.1535345e-02 1.3622909e-02 5.0972056e-02 5.9404154e-03 6.2723253e-03
  8.5713518e-01 4.8242141e-02 6.2796236e-03]
 [1.6988854e-01 9.4413852e-05 3.1960518e-03 4.1853622e-02 1.6049269e-03
  6.0502114e-04 3.5303605e-01 4.2972136e-01]
 [8.7867761e-03 8.7369986e-02 3.8823066e-05 7.6177745e-04 6.9632574e-07
  3.4140390e-07 9.0275645e-01 2.8515590e-04]
 [2.4157202e-04 5.5719984e-06 1.3277222e-03 3.0065286e-03 9.9475384e-01
  4.3582004e-05 2.9723020e-04 3.2391766e-04]
 [9.9270564e-01 7.3662882e-06 1.9507855e-04 3.7164380e-05 3.2078145e-07
  2.2515986e-05 3.3822297e-03 3.6496273e-03]
 [1.5884548e-04 4.8778875e-05 2.0851897e-02 4.7885776e-03 9.7371912e-01
  1.3803419e-06 1.8931001e-04 2.4213307e-04]
 [3.0793191e-03 4.1489337e-05 2.1340635e-03 4.0563889e-04 1.2792607e-03
  2.5162613e-03 4.0024149e-01 5.9030247e-01]
 [9.7298682e-01 3.9966919e-05 5.6861492e-04 7.1073440e-04 5.2820692e-06
  4.0997471e-05 1.2551623e-02 1.3095890e-02]
 [1.5562119e-02 7.0907664e-03 2.1412231e-01 1.1213302e-01 6.3143438e-01
  7.4185467e-05 6.9970940e-03 1.2586118e-02]
 [3.5562448e-03 8.1221962e-01 2.4903873e-03 8.5632261e-03 4.2573680e-04
  4.1390969e-05 1.7264460e-01 5.8879450e-05]
 [9.9439764e-01 4.6879774e-07 5.6662277e-04 3.3812412e-05 2.2839882e-07
  9.9782017e-07 3.5051480e-03 1.4950335e-03]
 [1.3026755e-02 6.4642797e-04 5.6883100e-02 6.8321250e-02 8.5326755e-01
  3.1089945e-05 4.9824105e-03 2.8414268e-03]
 [1.1200990e-05 6.4440869e-06 8.1567356e-05 1.1588449e-04 2.0245067e-04
  9.9949861e-01 1.0411230e-05 7.3401199e-05]
 [2.0759675e-01 3.4547428e-04 1.3845500e-02 1.5089491e-01 5.2126098e-01
  6.0368777e-04 5.7731826e-02 4.7720838e-02]
 [2.1782976e-06 1.5031048e-08 1.7211189e-06 1.2689385e-09 4.9221285e-09
  9.9999368e-01 2.0351345e-06 3.5843377e-07]
 [1.6999535e-02 2.1758620e-03 4.1000787e-02 1.9614652e-02 7.3397827e-01
  2.2259480e-04 1.4257792e-01 4.3430425e-02]
 [7.1733423e-02 2.9537309e-04 1.8009466e-01 1.1239039e-01 6.1611700e-01
  5.6043969e-05 1.5984142e-02 3.3289040e-03]
 [5.1523186e-02 2.3174843e-01 3.0465396e-03 5.3290691e-04 5.4548487e-05
  3.2132562e-06 7.0470190e-01 8.3893780e-03]]
[[9.7414613e-01 5.9112109e-04 1.7689185e-03 2.2762164e-03 1.4040008e-05
  5.5915214e-05 1.0072201e-02 1.1075475e-02]
 [3.8914796e-02 1.2456113e-03 1.4001562e-01 7.4901158e-01 5.4608461e-02
  2.0241843e-05 1.3232293e-02 2.9514425e-03]
 [1.1535345e-02 1.3622909e-02 5.0972056e-02 5.9404154e-03 6.2723253e-03
  8.5713518e-01 4.8242141e-02 6.2796236e-03]
 [1.6988854e-01 9.4413852e-05 3.1960518e-03 4.1853622e-02 1.6049269e-03
  6.0502114e-04 3.5303605e-01 4.2972136e-01]
 [8.7867761e-03 8.7369986e-02 3.8823066e-05 7.6177745e-04 6.9632574e-07
  3.4140390e-07 9.0275645e-01 2.8515590e-04]
 [2.4157202e-04 5.5719984e-06 1.3277222e-03 3.0065286e-03 9.9475384e-01
  4.3582004e-05 2.9723020e-04 3.2391766e-04]
 [9.9270564e-01 7.3662882e-06 1.9507855e-04 3.7164380e-05 3.2078145e-07
  2.2515986e-05 3.3822297e-03 3.6496273e-03]
 [1.5884548e-04 4.8778875e-05 2.0851897e-02 4.7885776e-03 9.7371912e-01
  1.3803419e-06 1.8931001e-04 2.4213307e-04]
 [3.0793191e-03 4.1489337e-05 2.1340635e-03 4.0563889e-04 1.2792607e-03
  2.5162613e-03 4.0024149e-01 5.9030247e-01]
 [9.7298682e-01 3.9966919e-05 5.6861492e-04 7.1073440e-04 5.2820692e-06
  4.0997471e-05 1.2551623e-02 1.3095890e-02]
 [1.5562119e-02 7.0907664e-03 2.1412231e-01 1.1213302e-01 6.3143438e-01
  7.4185467e-05 6.9970940e-03 1.2586118e-02]
 [3.5562448e-03 8.1221962e-01 2.4903873e-03 8.5632261e-03 4.2573680e-04
  4.1390969e-05 1.7264460e-01 5.8879450e-05]
 [9.9439764e-01 4.6879774e-07 5.6662277e-04 3.3812412e-05 2.2839882e-07
  9.9782017e-07 3.5051480e-03 1.4950335e-03]
 [1.3026755e-02 6.4642797e-04 5.6883100e-02 6.8321250e-02 8.5326755e-01
  3.1089945e-05 4.9824105e-03 2.8414268e-03]
 [1.1200990e-05 6.4440869e-06 8.1567356e-05 1.1588449e-04 2.0245067e-04
  9.9949861e-01 1.0411230e-05 7.3401199e-05]
 [2.0759675e-01 3.4547428e-04 1.3845500e-02 1.5089491e-01 5.2126098e-01
  6.0368777e-04 5.7731826e-02 4.7720838e-02]
 [2.1782976e-06 1.5031048e-08 1.7211189e-06 1.2689385e-09 4.9221285e-09
  9.9999368e-01 2.0351345e-06 3.5843377e-07]
 [1.6999535e-02 2.1758620e-03 4.1000787e-02 1.9614652e-02 7.3397827e-01
  2.2259480e-04 1.4257792e-01 4.3430425e-02]
 [7.1733423e-02 2.9537309e-04 1.8009466e-01 1.1239039e-01 6.1611700e-01
  5.6043969e-05 1.5984142e-02 3.3289040e-03]
 [5.1523186e-02 2.3174843e-01 3.0465396e-03 5.3290691e-04 5.4548487e-05
  3.2132562e-06 7.0470190e-01 8.3893780e-03]]
[[1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]]
Best Validation Accuracy: 0.7720093526030141
Current Validation Accuracy: 0.7730580497984213
Current Validation Loss: 0.00031230348721197134
Saving best model based on accuracy
Saving last model
