Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
A conda environment has been detected CONDA_PREFIX=
/sw/external/python/anaconda3 
anaconda3_gpu is loaded. Consider running conda deactivate and reloading it.

Currently Loaded Modules:
  1) cue-login-env/1.0   4) gcc/11.2.0    7) openmpi/4.1.2
  2) default             5) cuda/11.6.1   8) cudnn/8.4.1.50
  3) modtree/gpu         6) ucx/1.11.2    9) anaconda3_gpu/4.13.0

 

/u/avroy/UQ_XAI_Studies/models/PFIN_UQ/model/jobsub
total 508K
-rw-rw----+ 1 avroy delta_bbhj  925 Mar 31 16:58 jobsubmitter_dummy.py
-rw-rw----+ 1 avroy delta_bbhj 5.2K May 27 21:16 jobsubmitter_jetclass_1gpu.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 27 21:04 jobsubmitter_jetclass_1gpu.py~
-rw-rw----+ 1 avroy delta_bbhj  743 May 19 01:04 jobsubmitter_jetclass.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 18 13:59 jobsubmitter.py
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 19 13:52 UQPFIN-jetclass-run-jetclass20M_0_baseline.slurm
-rw-------+ 1 avroy delta_bbhj  973 May 19 00:59 UQPFIN-jetclass-run.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_9.slurm
-rw-------+ 1 avroy delta_bbhj  969 May 15 09:44 UQPFIN-run.slurm
/projects/bbhj/avroy/UQ_XAI_Studies/models/PFIN_UQ/model
total 184K
drwxr-s---+ 2 avroy delta_bbhj  12K May 28 01:44 jobouts
drwxr-s---+ 2 avroy delta_bbhj  16K May 27 21:18 jobsub
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 18 21:37 PFINDataset.py
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 15 14:46 PFINDataset.py~
drwxrws---+ 2 avroy delta_bbhj 4.0K May 18 21:38 __pycache__
drwxrws---+ 2 avroy delta_bbhj  16K May 27 23:34 trained_model_dicts
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:09 trained_model_dicts_old
drwxrws---+ 2 avroy delta_bbhj  24K May 27 23:53 trained_models
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:10 trained_models_old
-rw-rw----+ 1 avroy delta_bbhj  23K May 18 15:52 train_mod.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 27 21:06 train.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 18 13:50 train.py~
-rw-rw----+ 1 avroy delta_bbhj  13K May  5 09:36 UQPFIN.py
job is starting on gpua023.delta.ncsa.illinois.edu
# conda environments:
#
base                     /sw/external/python/anaconda3
toptagger_env         *  /u/avroy/.conda/envs/toptagger_env

/sw/external/python/anaconda3_gpu/bin/python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UQPFIN                                   [1, 10]                   --
├─Sequential: 1-1                        [60, 64]                  --
│    └─Sequential: 2-1                   [60, 100]                 --
│    │    └─Linear: 3-1                  [60, 100]                 1,200
│    │    └─ReLU: 3-2                    [60, 100]                 --
│    └─Sequential: 2-2                   [60, 100]                 --
│    │    └─Linear: 3-3                  [60, 100]                 10,100
│    │    └─ReLU: 3-4                    [60, 100]                 --
│    └─Sequential: 2-3                   [60, 64]                  --
│    │    └─Linear: 3-5                  [60, 64]                  6,464
│    │    └─ReLU: 3-6                    [60, 64]                  --
├─Sequential: 1-2                        [1770, 64]                --
│    └─Sequential: 2-4                   [1770, 128]               --
│    │    └─Linear: 3-7                  [1770, 128]               640
│    │    └─ReLU: 3-8                    [1770, 128]               --
│    └─Sequential: 2-5                   [1770, 128]               --
│    │    └─Linear: 3-9                  [1770, 128]               16,512
│    │    └─ReLU: 3-10                   [1770, 128]               --
│    └─Sequential: 2-6                   [1770, 64]                --
│    │    └─Linear: 3-11                 [1770, 64]                8,256
│    │    └─ReLU: 3-12                   [1770, 64]                --
├─Sequential: 1-3                        [60, 64]                  --
│    └─Sequential: 2-7                   [60, 128]                 --
│    │    └─Linear: 3-13                 [60, 128]                 9,728
│    │    └─ReLU: 3-14                   [60, 128]                 --
│    └─Sequential: 2-8                   [60, 128]                 --
│    │    └─Linear: 3-15                 [60, 128]                 16,512
│    │    └─ReLU: 3-16                   [60, 128]                 --
│    └─Sequential: 2-9                   [60, 64]                  --
│    │    └─Linear: 3-17                 [60, 64]                  8,256
│    │    └─ReLU: 3-18                   [60, 64]                  --
├─Sequential: 1-4                        [1, 10]                   --
│    └─Sequential: 2-10                  [1, 64]                   --
│    │    └─Linear: 3-19                 [1, 64]                   4,160
│    │    └─ReLU: 3-20                   [1, 64]                   --
│    └─Sequential: 2-11                  [1, 100]                  --
│    │    └─Linear: 3-21                 [1, 100]                  6,500
│    │    └─ReLU: 3-22                   [1, 100]                  --
│    └─Sequential: 2-12                  [1, 100]                  --
│    │    └─Linear: 3-23                 [1, 100]                  10,100
│    │    └─ReLU: 3-24                   [1, 100]                  --
│    └─Linear: 2-13                      [1, 10]                   1,010
│    └─Softmax: 2-14                     [1, 10]                   --
==========================================================================================
Total params: 99,438
Trainable params: 99,438
Non-trainable params: 0
Total mult-adds (M): 48.13
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 4.81
Params size (MB): 0.40
Estimated Total Size (MB): 5.21
==========================================================================================
data type:  jetclass
m_logic, pt_logic, eta_logic:  AND AND AND
m1, m2, pt1, pt2, eta1, eta2:  0.0 10000.0 0.0 10000.0 -6.0 6.0
skip labels:  []
classes:  10
Epoch 0
Best Validation Accuracy: 0
Current Validation Accuracy: 0.7059530020624188
Current Validation Loss: 0.00032850779603750584
Saving best model based on accuracy
Epoch 1
Best Validation Accuracy: 0.7059530020624188
Current Validation Accuracy: 0.7226584443369521
Current Validation Loss: 0.0003105315804827785
Saving best model based on accuracy
Epoch 2
Best Validation Accuracy: 0.7226584443369521
Current Validation Accuracy: 0.7272399854135018
Current Validation Loss: 0.00030479970849384694
Saving best model based on accuracy
Epoch 3
Best Validation Accuracy: 0.7272399854135018
Current Validation Accuracy: 0.7312107543910812
Current Validation Loss: 0.00030069752272232874
Saving best model based on accuracy
Epoch 4
Best Validation Accuracy: 0.7312107543910812
Current Validation Accuracy: 0.7328294755313493
Current Validation Loss: 0.0002989812139724022
Saving best model based on accuracy
Epoch 5
Best Validation Accuracy: 0.7328294755313493
Current Validation Accuracy: 0.7355581911741681
Current Validation Loss: 0.0002963462144203499
Saving best model based on accuracy
Epoch 6
Best Validation Accuracy: 0.7355581911741681
Current Validation Accuracy: 0.7361459530220713
Current Validation Loss: 0.00029520036675100887
Saving best model based on accuracy
Epoch 7
Best Validation Accuracy: 0.7361459530220713
Current Validation Accuracy: 0.73670220083047
Current Validation Loss: 0.0002946570505726395
Saving best model based on accuracy
Epoch 8
Best Validation Accuracy: 0.73670220083047
Current Validation Accuracy: 0.7371048802241399
Current Validation Loss: 0.00029416026650619045
Saving best model based on accuracy
Epoch 9
Best Validation Accuracy: 0.7371048802241399
Current Validation Accuracy: 0.7375300696460273
Current Validation Loss: 0.0002936470411609128
Saving best model based on accuracy
Epoch 10
Best Validation Accuracy: 0.7375300696460273
Current Validation Accuracy: 0.7398065838330976
Current Validation Loss: 0.00029114868175717776
Saving best model based on accuracy
Epoch 11
Best Validation Accuracy: 0.7398065838330976
Current Validation Accuracy: 0.762072003077371
Current Validation Loss: 0.00026873919286365293
Saving best model based on accuracy
Epoch 12
Best Validation Accuracy: 0.762072003077371
Current Validation Accuracy: 0.7654159928248034
Current Validation Loss: 0.0002646833975202991
Saving best model based on accuracy
Epoch 13
Best Validation Accuracy: 0.7654159928248034
Current Validation Accuracy: 0.7714982024491911
Current Validation Loss: 0.0002578441376671687
Saving best model based on accuracy
Epoch 14
Best Validation Accuracy: 0.7714982024491911
Current Validation Accuracy: 0.7691251452522099
Current Validation Loss: 0.00026040044053923317
Epoch 15
Best Validation Accuracy: 0.7714982024491911
Current Validation Accuracy: 0.7726567185681221
Current Validation Loss: 0.0002566399721349432
Saving best model based on accuracy
Epoch 16
Best Validation Accuracy: 0.7726567185681221
Current Validation Accuracy: 0.779679347149155
Current Validation Loss: 0.0002484248468186714
Saving best model based on accuracy
Epoch 17
Best Validation Accuracy: 0.779679347149155
Current Validation Accuracy: 0.7835010497176492
Current Validation Loss: 0.0002444989056813291
Saving best model based on accuracy
Epoch 18
Best Validation Accuracy: 0.7835010497176492
Current Validation Accuracy: 0.7831789062027134
Current Validation Loss: 0.00024453735400746323
Epoch 19
Best Validation Accuracy: 0.7835010497176492
Current Validation Accuracy: 0.7896642954436202
Current Validation Loss: 0.00023748717012505
Saving best model based on accuracy
Epoch 20
Best Validation Accuracy: 0.7896642954436202
Current Validation Accuracy: 0.7827337078668547
Current Validation Loss: 0.0002450882543813375
Epoch 21
Best Validation Accuracy: 0.7896642954436202
Current Validation Accuracy: 0.7903365949530515
Current Validation Loss: 0.00023645185948887289
Saving best model based on accuracy
Epoch 22
Best Validation Accuracy: 0.7903365949530515
Current Validation Accuracy: 0.7917557271764571
Current Validation Loss: 0.00023504795203158492
Saving best model based on accuracy
Epoch 23
Best Validation Accuracy: 0.7917557271764571
Current Validation Accuracy: 0.7912064824879483
Current Validation Loss: 0.00023544045914934618
Epoch 24
Best Validation Accuracy: 0.7917557271764571
Current Validation Accuracy: 0.7864213507117421
Current Validation Loss: 0.0002411319477495479
Epoch 25
Best Validation Accuracy: 0.7917557271764571
Current Validation Accuracy: 0.7919223013852671
Current Validation Loss: 0.00023455834422352986
Saving best model based on accuracy
Epoch 26
Best Validation Accuracy: 0.7919223013852671
Current Validation Accuracy: 0.7932538946100488
Current Validation Loss: 0.00023294704156291723
Saving best model based on accuracy
Epoch 27
Best Validation Accuracy: 0.7932538946100488
Current Validation Accuracy: 0.7941713033156271
Current Validation Loss: 0.00023199117117810706
Saving best model based on accuracy
Epoch 28
Best Validation Accuracy: 0.7941713033156271
Current Validation Accuracy: 0.794351883764217
Current Validation Loss: 0.0002319098211775933
Saving best model based on accuracy
Epoch 29
[[9.80111897e-01 1.34825154e-04 9.02358792e-04 1.08477927e-03
  2.92931054e-05 8.70265067e-05 8.53450876e-03 9.05892253e-03
  4.43581848e-06 5.18729576e-05]
 [1.66601893e-02 7.12936278e-04 1.38943672e-01 7.66723335e-01
  6.76547661e-02 6.81293250e-06 7.15859141e-03 1.47558469e-03
  6.62417966e-04 1.70812928e-06]
 [4.44158150e-06 1.40481161e-05 1.09411849e-05 6.13698603e-08
  5.45376355e-10 8.42057518e-04 2.04706407e-06 2.01160319e-07
  2.62009451e-07 9.99125898e-01]
 [9.48239118e-03 2.83948099e-03 2.98102144e-02 2.50765588e-03
  2.40017218e-03 8.36408436e-01 1.03593878e-01 1.21626975e-02
  5.96209255e-04 1.98768350e-04]
 [4.73533390e-08 1.98812018e-06 1.07055440e-08 6.04259620e-09
  5.86138429e-11 3.97382013e-04 7.82998768e-08 7.18045290e-10
  2.03282383e-07 9.99600232e-01]
 [1.68254897e-01 2.66973831e-04 3.44650820e-03 3.13689709e-02
  1.70686620e-03 4.00182616e-04 3.90467465e-01 4.04021591e-01
  5.82075118e-05 8.40346365e-06]
 [1.26458360e-02 1.90716758e-01 3.00082465e-05 1.93073275e-03
  1.01752403e-06 2.53800039e-08 7.94109225e-01 7.84880685e-05
  3.91128910e-04 9.67718734e-05]
 [3.00538231e-04 4.48555875e-06 2.00978294e-03 6.12567645e-03
  9.90618408e-01 2.45122974e-05 2.59444350e-04 5.37937914e-04
  1.19141470e-04 6.18835969e-08]
 [9.96867239e-01 4.43395902e-06 1.09738976e-04 2.45818192e-05
  7.78303786e-07 1.14294220e-04 1.49957417e-03 1.36220769e-03
  7.16915451e-07 1.64370886e-05]
 [2.45831441e-04 8.83008688e-05 1.99018586e-02 7.61549594e-03
  9.70057130e-01 6.77337766e-06 1.59630770e-04 5.25775715e-04
  1.39713543e-03 1.92909124e-06]
 [9.10586119e-03 1.18298558e-04 2.28140107e-03 2.70236027e-03
  1.12742977e-02 1.80796406e-03 4.51115280e-01 5.21544039e-01
  4.85685741e-05 1.91641561e-06]
 [9.63327289e-01 6.71856906e-05 7.45806785e-04 5.22225513e-04
  1.23021618e-05 5.17966073e-05 1.84466466e-02 1.68102067e-02
  3.81334416e-06 1.27264921e-05]
 [1.47356912e-02 5.98121900e-04 6.52330518e-02 1.79508001e-01
  7.00293481e-01 2.63086513e-05 9.84641723e-03 1.23864962e-02
  1.73706040e-02 1.83037571e-06]
 [5.82304876e-03 4.91454780e-01 4.34076553e-03 7.18980283e-03
  5.78075065e-04 1.53305973e-05 4.89569128e-01 3.60099686e-04
  6.63640094e-04 5.35153913e-06]
 [9.86329138e-01 8.93913057e-06 1.24326104e-03 6.23435917e-05
  1.94786639e-06 6.82142036e-06 6.41036872e-03 5.43248281e-03
  4.87283716e-04 1.74369761e-05]
 [3.11269332e-02 1.46669016e-04 2.80911159e-02 1.17655002e-01
  7.79998541e-01 7.16850918e-05 1.16207572e-02 1.09956982e-02
  2.02883296e-02 5.32742979e-06]
 [1.20603172e-05 2.80363196e-07 2.10657672e-05 2.61810692e-05
  6.69371075e-05 9.99757946e-01 1.77970214e-05 4.02675541e-05
  3.12661746e-06 5.42776252e-05]
 [6.34089634e-02 3.23563836e-05 1.55788218e-03 2.49775816e-02
  2.41810799e-01 2.69925309e-04 2.97559071e-02 2.79252995e-02
  6.10253274e-01 8.03483726e-06]
 [1.08827685e-03 5.03991172e-03 8.03874154e-03 7.21269578e-04
  7.87886675e-04 3.52958814e-05 1.99118047e-03 1.50776096e-03
  9.80760098e-01 2.95902300e-05]
 [5.16148418e-07 4.70090278e-09 1.29739544e-06 6.51141576e-08
  2.65271950e-07 9.94866252e-01 4.44960051e-07 8.38613346e-07
  1.01387336e-07 5.13010472e-03]]
[[9.80111897e-01 1.34825154e-04 9.02358792e-04 1.08477927e-03
  2.92931054e-05 8.70265067e-05 8.53450876e-03 9.05892253e-03
  4.43581848e-06 5.18729576e-05]
 [1.66601893e-02 7.12936278e-04 1.38943672e-01 7.66723335e-01
  6.76547661e-02 6.81293250e-06 7.15859141e-03 1.47558469e-03
  6.62417966e-04 1.70812928e-06]
 [4.44158150e-06 1.40481161e-05 1.09411849e-05 6.13698603e-08
  5.45376355e-10 8.42057518e-04 2.04706407e-06 2.01160319e-07
  2.62009451e-07 9.99125898e-01]
 [9.48239118e-03 2.83948099e-03 2.98102144e-02 2.50765588e-03
  2.40017218e-03 8.36408436e-01 1.03593878e-01 1.21626975e-02
  5.96209255e-04 1.98768350e-04]
 [4.73533390e-08 1.98812018e-06 1.07055440e-08 6.04259620e-09
  5.86138429e-11 3.97382013e-04 7.82998768e-08 7.18045290e-10
  2.03282383e-07 9.99600232e-01]
 [1.68254897e-01 2.66973831e-04 3.44650820e-03 3.13689709e-02
  1.70686620e-03 4.00182616e-04 3.90467465e-01 4.04021591e-01
  5.82075118e-05 8.40346365e-06]
 [1.26458360e-02 1.90716758e-01 3.00082465e-05 1.93073275e-03
  1.01752403e-06 2.53800039e-08 7.94109225e-01 7.84880685e-05
  3.91128910e-04 9.67718734e-05]
 [3.00538231e-04 4.48555875e-06 2.00978294e-03 6.12567645e-03
  9.90618408e-01 2.45122974e-05 2.59444350e-04 5.37937914e-04
  1.19141470e-04 6.18835969e-08]
 [9.96867239e-01 4.43395902e-06 1.09738976e-04 2.45818192e-05
  7.78303786e-07 1.14294220e-04 1.49957417e-03 1.36220769e-03
  7.16915451e-07 1.64370886e-05]
 [2.45831441e-04 8.83008688e-05 1.99018586e-02 7.61549594e-03
  9.70057130e-01 6.77337766e-06 1.59630770e-04 5.25775715e-04
  1.39713543e-03 1.92909124e-06]
 [9.10586119e-03 1.18298558e-04 2.28140107e-03 2.70236027e-03
  1.12742977e-02 1.80796406e-03 4.51115280e-01 5.21544039e-01
  4.85685741e-05 1.91641561e-06]
 [9.63327289e-01 6.71856906e-05 7.45806785e-04 5.22225513e-04
  1.23021618e-05 5.17966073e-05 1.84466466e-02 1.68102067e-02
  3.81334416e-06 1.27264921e-05]
 [1.47356912e-02 5.98121900e-04 6.52330518e-02 1.79508001e-01
  7.00293481e-01 2.63086513e-05 9.84641723e-03 1.23864962e-02
  1.73706040e-02 1.83037571e-06]
 [5.82304876e-03 4.91454780e-01 4.34076553e-03 7.18980283e-03
  5.78075065e-04 1.53305973e-05 4.89569128e-01 3.60099686e-04
  6.63640094e-04 5.35153913e-06]
 [9.86329138e-01 8.93913057e-06 1.24326104e-03 6.23435917e-05
  1.94786639e-06 6.82142036e-06 6.41036872e-03 5.43248281e-03
  4.87283716e-04 1.74369761e-05]
 [3.11269332e-02 1.46669016e-04 2.80911159e-02 1.17655002e-01
  7.79998541e-01 7.16850918e-05 1.16207572e-02 1.09956982e-02
  2.02883296e-02 5.32742979e-06]
 [1.20603172e-05 2.80363196e-07 2.10657672e-05 2.61810692e-05
  6.69371075e-05 9.99757946e-01 1.77970214e-05 4.02675541e-05
  3.12661746e-06 5.42776252e-05]
 [6.34089634e-02 3.23563836e-05 1.55788218e-03 2.49775816e-02
  2.41810799e-01 2.69925309e-04 2.97559071e-02 2.79252995e-02
  6.10253274e-01 8.03483726e-06]
 [1.08827685e-03 5.03991172e-03 8.03874154e-03 7.21269578e-04
  7.87886675e-04 3.52958814e-05 1.99118047e-03 1.50776096e-03
  9.80760098e-01 2.95902300e-05]
 [5.16148418e-07 4.70090278e-09 1.29739544e-06 6.51141576e-08
  2.65271950e-07 9.94866252e-01 4.44960051e-07 8.38613346e-07
  1.01387336e-07 5.13010472e-03]]
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
Best Validation Accuracy: 0.794351883764217
Current Validation Accuracy: 0.7960971612853526
Current Validation Loss: 0.00023001122330565626
Saving best model based on accuracy
Saving last model
