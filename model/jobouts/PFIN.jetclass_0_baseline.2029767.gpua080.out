Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
A conda environment has been detected CONDA_PREFIX=
/sw/external/python/anaconda3 
anaconda3_gpu is loaded. Consider running conda deactivate and reloading it.

Currently Loaded Modules:
  1) cue-login-env/1.0   4) gcc/11.2.0    7) openmpi/4.1.2
  2) default             5) cuda/11.6.1   8) cudnn/8.4.1.50
  3) modtree/gpu         6) ucx/1.11.2    9) anaconda3_gpu/4.13.0

 

/u/avroy/UQ_XAI_Studies/models/PFIN_UQ/model/jobsub
total 508K
-rw-rw----+ 1 avroy delta_bbhj  925 Mar 31 16:58 jobsubmitter_dummy.py
-rw-rw----+ 1 avroy delta_bbhj 5.2K May 27 21:16 jobsubmitter_jetclass_1gpu.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 27 21:04 jobsubmitter_jetclass_1gpu.py~
-rw-rw----+ 1 avroy delta_bbhj  743 May 19 01:04 jobsubmitter_jetclass.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 18 13:59 jobsubmitter.py
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 19 13:52 UQPFIN-jetclass-run-jetclass20M_0_baseline.slurm
-rw-------+ 1 avroy delta_bbhj  973 May 19 00:59 UQPFIN-jetclass-run.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_9.slurm
-rw-------+ 1 avroy delta_bbhj  969 May 15 09:44 UQPFIN-run.slurm
/projects/bbhj/avroy/UQ_XAI_Studies/models/PFIN_UQ/model
total 204K
drwxr-s---+ 2 avroy delta_bbhj  16K May 28 19:31 jobouts
drwxr-s---+ 2 avroy delta_bbhj  16K May 27 21:18 jobsub
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 18 21:37 PFINDataset.py
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 15 14:46 PFINDataset.py~
drwxrws---+ 2 avroy delta_bbhj 4.0K May 18 21:38 __pycache__
drwxrws---+ 2 avroy delta_bbhj  20K May 28 18:06 trained_model_dicts
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:09 trained_model_dicts_old
drwxrws---+ 2 avroy delta_bbhj  36K May 28 19:31 trained_models
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:10 trained_models_old
-rw-rw----+ 1 avroy delta_bbhj  23K May 18 15:52 train_mod.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 27 21:06 train.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 18 13:50 train.py~
-rw-rw----+ 1 avroy delta_bbhj  13K May  5 09:36 UQPFIN.py
job is starting on gpua080.delta.ncsa.illinois.edu
# conda environments:
#
base                     /sw/external/python/anaconda3
toptagger_env         *  /u/avroy/.conda/envs/toptagger_env

/sw/external/python/anaconda3_gpu/bin/python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UQPFIN                                   [1, 10]                   --
├─Sequential: 1-1                        [60, 64]                  --
│    └─Sequential: 2-1                   [60, 100]                 --
│    │    └─Linear: 3-1                  [60, 100]                 1,200
│    │    └─ReLU: 3-2                    [60, 100]                 --
│    └─Sequential: 2-2                   [60, 100]                 --
│    │    └─Linear: 3-3                  [60, 100]                 10,100
│    │    └─ReLU: 3-4                    [60, 100]                 --
│    └─Sequential: 2-3                   [60, 64]                  --
│    │    └─Linear: 3-5                  [60, 64]                  6,464
│    │    └─ReLU: 3-6                    [60, 64]                  --
├─Sequential: 1-2                        [1770, 64]                --
│    └─Sequential: 2-4                   [1770, 128]               --
│    │    └─Linear: 3-7                  [1770, 128]               640
│    │    └─ReLU: 3-8                    [1770, 128]               --
│    └─Sequential: 2-5                   [1770, 128]               --
│    │    └─Linear: 3-9                  [1770, 128]               16,512
│    │    └─ReLU: 3-10                   [1770, 128]               --
│    └─Sequential: 2-6                   [1770, 64]                --
│    │    └─Linear: 3-11                 [1770, 64]                8,256
│    │    └─ReLU: 3-12                   [1770, 64]                --
├─Sequential: 1-3                        [60, 64]                  --
│    └─Sequential: 2-7                   [60, 128]                 --
│    │    └─Linear: 3-13                 [60, 128]                 9,728
│    │    └─ReLU: 3-14                   [60, 128]                 --
│    └─Sequential: 2-8                   [60, 128]                 --
│    │    └─Linear: 3-15                 [60, 128]                 16,512
│    │    └─ReLU: 3-16                   [60, 128]                 --
│    └─Sequential: 2-9                   [60, 64]                  --
│    │    └─Linear: 3-17                 [60, 64]                  8,256
│    │    └─ReLU: 3-18                   [60, 64]                  --
├─Sequential: 1-4                        [1, 10]                   --
│    └─Sequential: 2-10                  [1, 64]                   --
│    │    └─Linear: 3-19                 [1, 64]                   4,160
│    │    └─ReLU: 3-20                   [1, 64]                   --
│    └─Sequential: 2-11                  [1, 100]                  --
│    │    └─Linear: 3-21                 [1, 100]                  6,500
│    │    └─ReLU: 3-22                   [1, 100]                  --
│    └─Sequential: 2-12                  [1, 100]                  --
│    │    └─Linear: 3-23                 [1, 100]                  10,100
│    │    └─ReLU: 3-24                   [1, 100]                  --
│    └─Linear: 2-13                      [1, 10]                   1,010
│    └─ReLU: 2-14                        [1, 10]                   --
==========================================================================================
Total params: 99,438
Trainable params: 99,438
Non-trainable params: 0
Total mult-adds (M): 48.13
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 4.81
Params size (MB): 0.40
Estimated Total Size (MB): 5.21
==========================================================================================
data type:  jetclass
m_logic, pt_logic, eta_logic:  AND AND AND
m1, m2, pt1, pt2, eta1, eta2:  0.0 10000.0 0.0 10000.0 -6.0 6.0
skip labels:  []
classes:  10
Epoch 0
Best Validation Accuracy: 0
Current Validation Accuracy: 0.5740697480727665
Current Validation Loss: 0.0002433806306190613
Saving best model based on accuracy
Epoch 1
Best Validation Accuracy: 0.5740697480727665
Current Validation Accuracy: 0.5831738039296507
Current Validation Loss: 0.00023583263236214437
Saving best model based on accuracy
Epoch 2
Best Validation Accuracy: 0.5831738039296507
Current Validation Accuracy: 0.5848100328696434
Current Validation Loss: 0.0002341039409230213
Saving best model based on accuracy
Epoch 3
Best Validation Accuracy: 0.5848100328696434
Current Validation Accuracy: 0.5838220927423167
Current Validation Loss: 0.0002329850365725893
Epoch 4
Best Validation Accuracy: 0.5848100328696434
Current Validation Accuracy: 0.632812918155038
Current Validation Loss: 0.00021423525642198563
Saving best model based on accuracy
Epoch 5
Best Validation Accuracy: 0.632812918155038
Current Validation Accuracy: 0.6332796260734157
Current Validation Loss: 0.00021356648710418633
Saving best model based on accuracy
Epoch 6
Best Validation Accuracy: 0.6332796260734157
Current Validation Accuracy: 0.6468016501351352
Current Validation Loss: 0.0001906053058299574
Saving best model based on accuracy
Epoch 7
Best Validation Accuracy: 0.6468016501351352
Current Validation Accuracy: 0.6499810665651548
Current Validation Loss: 0.00018773298613551618
Saving best model based on accuracy
Epoch 8
Best Validation Accuracy: 0.6499810665651548
Current Validation Accuracy: 0.6502952065145022
Current Validation Loss: 0.0001873597255872493
Saving best model based on accuracy
Epoch 9
Best Validation Accuracy: 0.6502952065145022
Current Validation Accuracy: 0.6505538217275796
Current Validation Loss: 0.00018714146630785758
Saving best model based on accuracy
Epoch 10
Best Validation Accuracy: 0.6505538217275796
Current Validation Accuracy: 0.6514222085939286
Current Validation Loss: 0.00018645905866137045
Saving best model based on accuracy
Epoch 11
Best Validation Accuracy: 0.6514222085939286
Current Validation Accuracy: 0.652345119750849
Current Validation Loss: 0.00018615948113508455
Saving best model based on accuracy
Epoch 12
Best Validation Accuracy: 0.652345119750849
Current Validation Accuracy: 0.687756895697033
Current Validation Loss: 0.00016801245294694538
Saving best model based on accuracy
Epoch 13
Best Validation Accuracy: 0.687756895697033
Current Validation Accuracy: 0.741751950493945
Current Validation Loss: 0.0001434693647688985
Saving best model based on accuracy
Epoch 14
Best Validation Accuracy: 0.741751950493945
Current Validation Accuracy: 0.7626427573483987
Current Validation Loss: 0.00013306493945941693
Saving best model based on accuracy
Epoch 15
Best Validation Accuracy: 0.7626427573483987
Current Validation Accuracy: 0.7616067958275412
Current Validation Loss: 0.0001336218424003849
Epoch 16
Best Validation Accuracy: 0.7626427573483987
Current Validation Accuracy: 0.7677795457876484
Current Validation Loss: 0.00013018870200935497
Saving best model based on accuracy
Epoch 17
Best Validation Accuracy: 0.7677795457876484
Current Validation Accuracy: 0.7705958004290911
Current Validation Loss: 0.00012856991992527428
Saving best model based on accuracy
Epoch 18
Best Validation Accuracy: 0.7705958004290911
Current Validation Accuracy: 0.769050111824818
Current Validation Loss: 0.00012945888854935927
Epoch 19
Best Validation Accuracy: 0.7705958004290911
Current Validation Accuracy: 0.7714516817242081
Current Validation Loss: 0.0001280904757799958
Saving best model based on accuracy
Epoch 20
Best Validation Accuracy: 0.7714516817242081
Current Validation Accuracy: 0.7735236047659232
Current Validation Loss: 0.00012698908703918823
Saving best model based on accuracy
Epoch 21
Best Validation Accuracy: 0.7735236047659232
Current Validation Accuracy: 0.7758186271984169
Current Validation Loss: 0.00012576082754618407
Saving best model based on accuracy
Epoch 22
Best Validation Accuracy: 0.7758186271984169
Current Validation Accuracy: 0.7777199742485277
Current Validation Loss: 0.00012475942773334988
Saving best model based on accuracy
Epoch 23
Best Validation Accuracy: 0.7777199742485277
Current Validation Accuracy: 0.7793471991772335
Current Validation Loss: 0.0001239127622204615
Saving best model based on accuracy
Epoch 24
Best Validation Accuracy: 0.7793471991772335
Current Validation Accuracy: 0.7833975035878484
Current Validation Loss: 0.00012176595831995324
Saving best model based on accuracy
Epoch 25
Best Validation Accuracy: 0.7833975035878484
Current Validation Accuracy: 0.7845310085643155
Current Validation Loss: 0.00012123901245927195
Saving best model based on accuracy
Epoch 26
Best Validation Accuracy: 0.7845310085643155
Current Validation Accuracy: 0.78591612563397
Current Validation Loss: 0.00012042815632311353
Saving best model based on accuracy
Epoch 27
Best Validation Accuracy: 0.78591612563397
Current Validation Accuracy: 0.7841003166910859
Current Validation Loss: 0.00012137456338389864
Epoch 28
Best Validation Accuracy: 0.78591612563397
Current Validation Accuracy: 0.7899284131080396
Current Validation Loss: 0.0001183782734738775
Saving best model based on accuracy
Epoch 29
[[2.76826147e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.77123528e+01 2.29799843e+01
  0.00000000e+00 0.00000000e+00]
 [4.00726395e+01 0.00000000e+00 4.01480530e+02 4.74981171e+02
  1.50446884e+02 0.00000000e+00 2.21590099e+01 2.88719463e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 7.97368213e+03]
 [8.40838623e+00 6.99310913e+01 2.87647003e+02 0.00000000e+00
  5.15524530e+00 5.34193359e+02 3.15729370e+02 4.49276848e+01
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 7.84970312e+03]
 [2.16421814e+02 0.00000000e+00 0.00000000e+00 3.32518501e+01
  0.00000000e+00 0.00000000e+00 5.10822723e+02 6.80858887e+02
  0.00000000e+00 0.00000000e+00]
 [8.50843048e+00 1.30967728e+02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 8.79299622e+02 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.24738623e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.32673584e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 2.21667242e+00 2.20606923e-01
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 2.92337189e+02 0.00000000e+00
  3.93601807e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.20150948e+01 0.00000000e+00 6.01247253e+02 9.68144165e+02
  0.00000000e+00 0.00000000e+00]
 [3.04016260e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 3.98376579e+01 2.01167164e+01
  0.00000000e+00 0.00000000e+00]
 [2.14013557e+01 0.00000000e+00 3.37595367e+02 2.19238708e+02
  1.03677039e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 8.55840515e+02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 5.40859314e+02 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [2.63565698e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [3.35543976e+01 0.00000000e+00 1.14517586e+02 2.84000671e+02
  1.79707629e+03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  2.38906765e+01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 6.04726611e+03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.90899521e+02 0.00000000e+00 0.00000000e+00 1.27962494e+02
  6.30678467e+02 0.00000000e+00 0.00000000e+00 3.45455284e+01
  1.28337622e+03 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 3.95327072e+01 1.51560726e+01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.99251480e+01
  4.38246045e+03 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 2.27030591e+03 0.00000000e+00 0.00000000e+00
  0.00000000e+00 9.98162994e+01]]
[[9.6862757e-01 3.4977830e-04 3.4977830e-04 3.4977830e-04 3.4977830e-04
  3.4977830e-04 2.0536307e-02 8.3876783e-03 3.4977830e-04 3.4977830e-04]
 [3.7270073e-02 9.0741849e-04 3.6521828e-01 4.3191412e-01 1.3742571e-01
  9.0741849e-04 2.1014914e-02 3.5273123e-03 9.0741849e-04 9.0741849e-04]
 [1.2525549e-04 1.2525549e-04 1.2525549e-04 1.2525549e-04 1.2525549e-04
  1.2525549e-04 1.2525549e-04 1.2525549e-04 1.2525549e-04 9.9887270e-01]
 [7.3733884e-03 5.5588968e-02 2.2621377e-01 7.8370387e-04 4.8238896e-03
  4.1943309e-01 2.4822202e-01 3.5993703e-02 7.8370387e-04 7.8370387e-04]
 [1.2723127e-04 1.2723127e-04 1.2723127e-04 1.2723127e-04 1.2723127e-04
  1.2723127e-04 1.2723127e-04 1.2723127e-04 1.2723127e-04 9.9885494e-01]
 [1.4980605e-01 6.8901113e-04 6.8901113e-04 2.3599906e-02 6.8901113e-04
  6.8901113e-04 3.5265157e-01 4.6980837e-01 6.8901113e-04 6.8901113e-04]
 [9.2424713e-03 1.2827648e-01 9.7202911e-04 9.7202911e-04 9.7202911e-04
  9.7202911e-04 8.5567689e-01 9.7202911e-04 9.7202911e-04 9.7202911e-04]
 [2.3488590e-04 2.3488590e-04 2.3488590e-04 2.3488590e-04 9.9788600e-01
  2.3488590e-04 2.3488590e-04 2.3488590e-04 2.3488590e-04 2.3488590e-04]
 [9.9657482e-01 2.9947533e-04 2.9947533e-04 2.9947533e-04 2.9947533e-04
  2.9947533e-04 9.6331409e-04 3.6554167e-04 2.9947533e-04 2.9947533e-04]
 [2.3594056e-04 2.3594056e-04 6.9210142e-02 2.3594056e-04 9.2890227e-01
  2.3594056e-04 2.3594056e-04 2.3594056e-04 2.3594056e-04 2.3594056e-04]
 [6.1674847e-04 6.1674847e-04 6.1674847e-04 6.1674847e-04 2.6529495e-02
  6.1674847e-04 3.7143508e-01 5.9771818e-01 6.1674847e-04 6.1674847e-04]
 [9.7782904e-01 3.2153132e-04 3.2153132e-04 3.2153132e-04 3.2153132e-04
  3.2153132e-04 1.3130586e-02 6.7896857e-03 3.2153132e-04 3.2153132e-04]
 [1.3785400e-02 6.1538239e-04 2.0836563e-01 1.3553102e-01 6.3862562e-01
  6.1538239e-04 6.1538239e-04 6.1538239e-04 6.1538239e-04 6.1538239e-04]
 [7.1088370e-04 6.0911399e-01 7.1088370e-04 7.1088370e-04 7.1088370e-04
  7.1088370e-04 3.8519895e-01 7.1088370e-04 7.1088370e-04 7.1088370e-04]
 [9.9659818e-01 3.7797794e-04 3.7797794e-04 3.7797794e-04 3.7797794e-04
  3.7797794e-04 3.7797794e-04 3.7797794e-04 3.7797794e-04 3.7797794e-04]
 [1.5269021e-02 4.4188357e-04 5.1045325e-02 1.2593712e-01 7.9454035e-01
  4.4188357e-04 4.4188357e-04 4.4188357e-04 1.0998781e-02 4.4188357e-04]
 [1.6509098e-04 1.6509098e-04 1.6509098e-04 1.6509098e-04 1.6509098e-04
  9.9851418e-01 1.6509098e-04 1.6509098e-04 1.6509098e-04 1.6509098e-04]
 [8.4260240e-02 4.3908518e-04 4.3908518e-04 5.6625519e-02 2.7736065e-01
  4.3908518e-04 4.3908518e-04 1.5607515e-02 5.6395060e-01 4.3908518e-04]
 [2.2236677e-04 2.2236677e-04 9.0131275e-03 3.5925738e-03 2.2236677e-04
  2.2236677e-04 2.2236677e-04 1.1324061e-02 9.7473598e-01 2.2236677e-04]
 [4.2014648e-04 4.2014648e-04 4.2014648e-04 4.2014648e-04 4.2014648e-04
  9.5428115e-01 4.2014648e-04 4.2014648e-04 4.2014648e-04 4.2357612e-02]]
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
Best Validation Accuracy: 0.7899284131080396
Current Validation Accuracy: 0.7891420627889725
Current Validation Loss: 0.00011872551261665712
Saving last model
