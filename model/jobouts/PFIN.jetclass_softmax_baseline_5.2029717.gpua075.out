Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
A conda environment has been detected CONDA_PREFIX=
/sw/external/python/anaconda3 
anaconda3_gpu is loaded. Consider running conda deactivate and reloading it.

Currently Loaded Modules:
  1) cue-login-env/1.0   4) gcc/11.2.0    7) openmpi/4.1.2
  2) default             5) cuda/11.6.1   8) cudnn/8.4.1.50
  3) modtree/gpu         6) ucx/1.11.2    9) anaconda3_gpu/4.13.0

 

/u/avroy/UQ_XAI_Studies/models/PFIN_UQ/model/jobsub
total 508K
-rw-rw----+ 1 avroy delta_bbhj  925 Mar 31 16:58 jobsubmitter_dummy.py
-rw-rw----+ 1 avroy delta_bbhj 5.2K May 27 21:16 jobsubmitter_jetclass_1gpu.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 27 21:04 jobsubmitter_jetclass_1gpu.py~
-rw-rw----+ 1 avroy delta_bbhj  743 May 19 01:04 jobsubmitter_jetclass.py
-rw-rw----+ 1 avroy delta_bbhj  15K May 18 13:59 jobsubmitter.py
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 19 13:52 UQPFIN-jetclass-run-jetclass20M_0_baseline.slurm
-rw-------+ 1 avroy delta_bbhj  973 May 19 00:59 UQPFIN-jetclass-run.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 27 21:18 UQPFIN-run-jetclass_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiph.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skiptwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiph_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skiptwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 27 21:18 UQPFIN-run-jetclass_softmax_skipwz_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0.1_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_0_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_nominal_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_baseline_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_baseline.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skiptop.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.2K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_dropout_skipwz.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skiptop_9.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_0.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_1.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_2.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_3.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_4.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_5.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_6.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_7.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_8.slurm
-rw-rw----+ 1 avroy delta_bbhj 1.1K May 18 14:02 UQPFIN-run-JNqgmerged_softmax_skipwz_9.slurm
-rw-------+ 1 avroy delta_bbhj  969 May 15 09:44 UQPFIN-run.slurm
/projects/bbhj/avroy/UQ_XAI_Studies/models/PFIN_UQ/model
total 184K
drwxr-s---+ 2 avroy delta_bbhj  12K May 28 01:44 jobouts
drwxr-s---+ 2 avroy delta_bbhj  16K May 27 21:18 jobsub
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 18 21:37 PFINDataset.py
-rw-rw----+ 1 avroy delta_bbhj 8.4K May 15 14:46 PFINDataset.py~
drwxrws---+ 2 avroy delta_bbhj 4.0K May 18 21:38 __pycache__
drwxrws---+ 2 avroy delta_bbhj  16K May 27 23:34 trained_model_dicts
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:09 trained_model_dicts_old
drwxrws---+ 2 avroy delta_bbhj  24K May 27 23:53 trained_models
drwxr-s---+ 2 avroy delta_bbhj 4.0K Mar 31 15:10 trained_models_old
-rw-rw----+ 1 avroy delta_bbhj  23K May 18 15:52 train_mod.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 27 21:06 train.py
-rw-rw----+ 1 avroy delta_bbhj  20K May 18 13:50 train.py~
-rw-rw----+ 1 avroy delta_bbhj  13K May  5 09:36 UQPFIN.py
job is starting on gpua075.delta.ncsa.illinois.edu
# conda environments:
#
base                     /sw/external/python/anaconda3
toptagger_env         *  /u/avroy/.conda/envs/toptagger_env

/sw/external/python/anaconda3_gpu/bin/python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
UQPFIN                                   [1, 10]                   --
├─Sequential: 1-1                        [60, 64]                  --
│    └─Sequential: 2-1                   [60, 100]                 --
│    │    └─Linear: 3-1                  [60, 100]                 1,200
│    │    └─ReLU: 3-2                    [60, 100]                 --
│    └─Sequential: 2-2                   [60, 100]                 --
│    │    └─Linear: 3-3                  [60, 100]                 10,100
│    │    └─ReLU: 3-4                    [60, 100]                 --
│    └─Sequential: 2-3                   [60, 64]                  --
│    │    └─Linear: 3-5                  [60, 64]                  6,464
│    │    └─ReLU: 3-6                    [60, 64]                  --
├─Sequential: 1-2                        [1770, 64]                --
│    └─Sequential: 2-4                   [1770, 128]               --
│    │    └─Linear: 3-7                  [1770, 128]               640
│    │    └─ReLU: 3-8                    [1770, 128]               --
│    └─Sequential: 2-5                   [1770, 128]               --
│    │    └─Linear: 3-9                  [1770, 128]               16,512
│    │    └─ReLU: 3-10                   [1770, 128]               --
│    └─Sequential: 2-6                   [1770, 64]                --
│    │    └─Linear: 3-11                 [1770, 64]                8,256
│    │    └─ReLU: 3-12                   [1770, 64]                --
├─Sequential: 1-3                        [60, 64]                  --
│    └─Sequential: 2-7                   [60, 128]                 --
│    │    └─Linear: 3-13                 [60, 128]                 9,728
│    │    └─ReLU: 3-14                   [60, 128]                 --
│    └─Sequential: 2-8                   [60, 128]                 --
│    │    └─Linear: 3-15                 [60, 128]                 16,512
│    │    └─ReLU: 3-16                   [60, 128]                 --
│    └─Sequential: 2-9                   [60, 64]                  --
│    │    └─Linear: 3-17                 [60, 64]                  8,256
│    │    └─ReLU: 3-18                   [60, 64]                  --
├─Sequential: 1-4                        [1, 10]                   --
│    └─Sequential: 2-10                  [1, 64]                   --
│    │    └─Linear: 3-19                 [1, 64]                   4,160
│    │    └─ReLU: 3-20                   [1, 64]                   --
│    └─Sequential: 2-11                  [1, 100]                  --
│    │    └─Linear: 3-21                 [1, 100]                  6,500
│    │    └─ReLU: 3-22                   [1, 100]                  --
│    └─Sequential: 2-12                  [1, 100]                  --
│    │    └─Linear: 3-23                 [1, 100]                  10,100
│    │    └─ReLU: 3-24                   [1, 100]                  --
│    └─Linear: 2-13                      [1, 10]                   1,010
│    └─Softmax: 2-14                     [1, 10]                   --
==========================================================================================
Total params: 99,438
Trainable params: 99,438
Non-trainable params: 0
Total mult-adds (M): 48.13
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 4.81
Params size (MB): 0.40
Estimated Total Size (MB): 5.21
==========================================================================================
data type:  jetclass
m_logic, pt_logic, eta_logic:  AND AND AND
m1, m2, pt1, pt2, eta1, eta2:  0.0 10000.0 0.0 10000.0 -6.0 6.0
skip labels:  []
classes:  10
Epoch 0
Best Validation Accuracy: 0
Current Validation Accuracy: 0.7086892210479768
Current Validation Loss: 0.000325805260382819
Saving best model based on accuracy
Epoch 1
Best Validation Accuracy: 0.7086892210479768
Current Validation Accuracy: 0.723564347916997
Current Validation Loss: 0.000309162016130386
Saving best model based on accuracy
Epoch 2
Best Validation Accuracy: 0.723564347916997
Current Validation Accuracy: 0.7289832620432403
Current Validation Loss: 0.00030336152951893167
Saving best model based on accuracy
Epoch 3
Best Validation Accuracy: 0.7289832620432403
Current Validation Accuracy: 0.7308936131046381
Current Validation Loss: 0.00030063884608826644
Saving best model based on accuracy
Epoch 4
Best Validation Accuracy: 0.7308936131046381
Current Validation Accuracy: 0.7331896359828304
Current Validation Loss: 0.0002982036697684321
Saving best model based on accuracy
Epoch 5
Best Validation Accuracy: 0.7331896359828304
Current Validation Accuracy: 0.734363659010089
Current Validation Loss: 0.00029675920709508133
Saving best model based on accuracy
Epoch 6
Best Validation Accuracy: 0.734363659010089
Current Validation Accuracy: 0.7356397274986006
Current Validation Loss: 0.00029536911940934686
Saving best model based on accuracy
Epoch 7
Best Validation Accuracy: 0.7356397274986006
Current Validation Accuracy: 0.7385195104419019
Current Validation Loss: 0.0002923974494371965
Saving best model based on accuracy
Epoch 8
Best Validation Accuracy: 0.7385195104419019
Current Validation Accuracy: 0.7625837310521838
Current Validation Loss: 0.00026845079110577127
Saving best model based on accuracy
Epoch 9
Best Validation Accuracy: 0.7625837310521838
Current Validation Accuracy: 0.7668036110087044
Current Validation Loss: 0.00026383201994915857
Saving best model based on accuracy
Epoch 10
Best Validation Accuracy: 0.7668036110087044
Current Validation Accuracy: 0.7700165423696257
Current Validation Loss: 0.00025991151744655244
Saving best model based on accuracy
Epoch 11
Best Validation Accuracy: 0.7700165423696257
Current Validation Accuracy: 0.7754109455762542
Current Validation Loss: 0.000253670207201074
Saving best model based on accuracy
Epoch 12
Best Validation Accuracy: 0.7754109455762542
Current Validation Accuracy: 0.7784142835633274
Current Validation Loss: 0.0002501357930531019
Saving best model based on accuracy
Epoch 13
Best Validation Accuracy: 0.7784142835633274
Current Validation Accuracy: 0.7813165765348463
Current Validation Loss: 0.00024679658432622383
Saving best model based on accuracy
Epoch 14
Best Validation Accuracy: 0.7813165765348463
Current Validation Accuracy: 0.7841558414273558
Current Validation Loss: 0.0002436369559736202
Saving best model based on accuracy
Epoch 15
Best Validation Accuracy: 0.7841558414273558
Current Validation Accuracy: 0.7797818928332572
Current Validation Loss: 0.0002485777425542279
Epoch 16
Best Validation Accuracy: 0.7841558414273558
Current Validation Accuracy: 0.7770841910070937
Current Validation Loss: 0.00025131817825588406
Epoch 17
Best Validation Accuracy: 0.7841558414273558
Current Validation Accuracy: 0.7886658506364586
Current Validation Loss: 0.00023841541748459847
Saving best model based on accuracy
Epoch 18
Best Validation Accuracy: 0.7886658506364586
Current Validation Accuracy: 0.7867735075976348
Current Validation Loss: 0.00024079396174568425
Epoch 19
Best Validation Accuracy: 0.7886658506364586
Current Validation Accuracy: 0.7894161849103776
Current Validation Loss: 0.00023761203641882973
Saving best model based on accuracy
Epoch 20
Best Validation Accuracy: 0.7894161849103776
Current Validation Accuracy: 0.7924265260173408
Current Validation Loss: 0.00023404683721121343
Saving best model based on accuracy
Epoch 21
Best Validation Accuracy: 0.7924265260173408
Current Validation Accuracy: 0.7932163778963528
Current Validation Loss: 0.00023342818782146765
Saving best model based on accuracy
Epoch 22
Best Validation Accuracy: 0.7932163778963528
Current Validation Accuracy: 0.7942923572451527
Current Validation Loss: 0.00023210339419430046
Saving best model based on accuracy
Epoch 23
Best Validation Accuracy: 0.7942923572451527
Current Validation Accuracy: 0.7931553507087408
Current Validation Loss: 0.00023344579557622004
Epoch 24
Best Validation Accuracy: 0.7942923572451527
Current Validation Accuracy: 0.7942788512282222
Current Validation Loss: 0.0002320131139131521
Epoch 25
Best Validation Accuracy: 0.7942923572451527
Current Validation Accuracy: 0.7957284970454337
Current Validation Loss: 0.00023012146426986015
Saving best model based on accuracy
Epoch 26
Best Validation Accuracy: 0.7957284970454337
Current Validation Accuracy: 0.7961661920385532
Current Validation Loss: 0.00022957977211769253
Saving best model based on accuracy
Epoch 27
Best Validation Accuracy: 0.7961661920385532
Current Validation Accuracy: 0.7969130247525272
Current Validation Loss: 0.00022891711474553475
Saving best model based on accuracy
Epoch 28
Best Validation Accuracy: 0.7969130247525272
Current Validation Accuracy: 0.7968174821883149
Current Validation Loss: 0.0002289107005441951
Validation Accuracy has not changed much, will stop in 4 epochs if this continues
Epoch 29
[[9.82921600e-01 1.55468952e-04 1.67835469e-03 1.44099910e-03
  2.44854782e-05 1.13418653e-04 7.10000237e-03 6.38304092e-03
  8.44210547e-07 1.81762982e-04]
 [1.98412705e-02 3.76210868e-04 1.17470928e-01 7.40844131e-01
  1.08265296e-01 3.96671931e-05 1.00371419e-02 2.27318308e-03
  8.50593380e-04 1.54687393e-06]
 [1.27655511e-07 2.42864667e-06 3.07543905e-06 2.28333494e-08
  1.37961531e-09 3.60068982e-03 2.90965062e-07 9.70695968e-09
  7.59894903e-09 9.96393502e-01]
 [3.40735144e-03 4.17618779e-03 2.46297028e-02 1.29077979e-03
  4.84503107e-03 9.23193812e-01 3.24315801e-02 5.19059645e-03
  6.19033526e-04 2.15906053e-04]
 [5.39737464e-08 4.88439923e-07 1.30905209e-09 4.82175633e-09
  2.90546406e-11 5.27820608e-04 8.18725425e-08 6.58080535e-11
  2.29256525e-08 9.99471605e-01]
 [1.18981548e-01 9.83150021e-05 3.64044239e-03 2.48198267e-02
  7.88457284e-04 5.69061958e-04 3.46071184e-01 5.05007684e-01
  2.08793790e-05 2.59323883e-06]
 [1.56477578e-02 1.21167064e-01 3.33624812e-05 1.46466319e-03
  8.35150502e-07 6.75254626e-08 8.61099303e-01 1.94639986e-04
  2.78068852e-04 1.14326511e-04]
 [5.56670129e-04 4.15963495e-06 1.87704596e-03 5.78652136e-03
  9.90627766e-01 5.88411494e-05 5.15426684e-04 3.42996209e-04
  2.30679332e-04 1.80598878e-08]
 [9.94361281e-01 1.38427195e-05 2.92215816e-04 2.92762961e-05
  6.92453511e-07 3.22818742e-05 2.47471593e-03 2.77965935e-03
  8.32107929e-08 1.58648854e-05]
 [8.22415095e-05 3.66438144e-05 1.94556993e-02 4.39330377e-03
  9.75206137e-01 7.88643683e-06 1.56111171e-04 2.17279259e-04
  4.44766425e-04 1.64844103e-08]
 [3.31185432e-03 1.27673862e-04 4.41352185e-03 1.13126531e-03
  5.13394177e-03 1.53423566e-03 3.31616849e-01 6.52651191e-01
  7.84348813e-05 1.06751020e-06]
 [9.79905248e-01 4.95578352e-05 7.00347009e-04 3.56501958e-04
  6.64912841e-06 7.25970822e-05 8.89774226e-03 9.99304466e-03
  5.49376637e-07 1.77068796e-05]
 [1.29854940e-02 2.37948005e-03 1.40257850e-01 1.20432593e-01
  6.65870011e-01 3.33568532e-05 1.24051869e-02 1.37071386e-02
  3.19247022e-02 4.12395639e-06]
 [2.50200182e-03 5.95974565e-01 6.88982895e-03 1.48611469e-02
  1.52046839e-03 2.23246516e-05 3.77550483e-01 1.60362382e-04
  5.15544496e-04 3.31728552e-06]
 [9.94613707e-01 7.41545136e-07 6.28760841e-04 7.62155323e-05
  4.51910182e-07 3.84655186e-06 2.92377663e-03 1.66005769e-03
  8.25389725e-05 9.90487933e-06]
 [8.23299866e-03 7.03133002e-04 4.30546217e-02 5.91966212e-02
  8.59538078e-01 9.54521820e-05 6.75338320e-03 4.18034149e-03
  1.82398222e-02 5.46405317e-06]
 [3.93521514e-05 8.05094260e-06 7.86758319e-05 4.02450787e-05
  3.99258715e-05 9.99613583e-01 4.49181789e-05 3.57692588e-05
  3.32891318e-06 9.62358172e-05]
 [6.46398738e-02 1.05187610e-04 2.06559384e-03 2.36271750e-02
  2.02959374e-01 4.31610300e-04 4.91256192e-02 2.55684238e-02
  6.31453574e-01 2.35435436e-05]
 [3.12692206e-03 1.25801889e-02 1.85741093e-02 5.09717828e-03
  1.39112361e-02 2.19145280e-04 9.96380392e-03 1.71764363e-02
  9.19216573e-01 1.34402420e-04]
 [4.71906333e-07 1.15334593e-07 9.66971243e-07 9.53821733e-09
  1.34414833e-07 9.70866144e-01 2.51898905e-06 7.56867962e-07
  1.68770910e-07 2.91285682e-02]]
[[9.82921600e-01 1.55468952e-04 1.67835469e-03 1.44099910e-03
  2.44854782e-05 1.13418653e-04 7.10000237e-03 6.38304092e-03
  8.44210547e-07 1.81762982e-04]
 [1.98412705e-02 3.76210868e-04 1.17470928e-01 7.40844131e-01
  1.08265296e-01 3.96671931e-05 1.00371419e-02 2.27318308e-03
  8.50593380e-04 1.54687393e-06]
 [1.27655511e-07 2.42864667e-06 3.07543905e-06 2.28333494e-08
  1.37961531e-09 3.60068982e-03 2.90965062e-07 9.70695968e-09
  7.59894903e-09 9.96393502e-01]
 [3.40735144e-03 4.17618779e-03 2.46297028e-02 1.29077979e-03
  4.84503107e-03 9.23193812e-01 3.24315801e-02 5.19059645e-03
  6.19033526e-04 2.15906053e-04]
 [5.39737464e-08 4.88439923e-07 1.30905209e-09 4.82175633e-09
  2.90546406e-11 5.27820608e-04 8.18725425e-08 6.58080535e-11
  2.29256525e-08 9.99471605e-01]
 [1.18981548e-01 9.83150021e-05 3.64044239e-03 2.48198267e-02
  7.88457284e-04 5.69061958e-04 3.46071184e-01 5.05007684e-01
  2.08793790e-05 2.59323883e-06]
 [1.56477578e-02 1.21167064e-01 3.33624812e-05 1.46466319e-03
  8.35150502e-07 6.75254626e-08 8.61099303e-01 1.94639986e-04
  2.78068852e-04 1.14326511e-04]
 [5.56670129e-04 4.15963495e-06 1.87704596e-03 5.78652136e-03
  9.90627766e-01 5.88411494e-05 5.15426684e-04 3.42996209e-04
  2.30679332e-04 1.80598878e-08]
 [9.94361281e-01 1.38427195e-05 2.92215816e-04 2.92762961e-05
  6.92453511e-07 3.22818742e-05 2.47471593e-03 2.77965935e-03
  8.32107929e-08 1.58648854e-05]
 [8.22415095e-05 3.66438144e-05 1.94556993e-02 4.39330377e-03
  9.75206137e-01 7.88643683e-06 1.56111171e-04 2.17279259e-04
  4.44766425e-04 1.64844103e-08]
 [3.31185432e-03 1.27673862e-04 4.41352185e-03 1.13126531e-03
  5.13394177e-03 1.53423566e-03 3.31616849e-01 6.52651191e-01
  7.84348813e-05 1.06751020e-06]
 [9.79905248e-01 4.95578352e-05 7.00347009e-04 3.56501958e-04
  6.64912841e-06 7.25970822e-05 8.89774226e-03 9.99304466e-03
  5.49376637e-07 1.77068796e-05]
 [1.29854940e-02 2.37948005e-03 1.40257850e-01 1.20432593e-01
  6.65870011e-01 3.33568532e-05 1.24051869e-02 1.37071386e-02
  3.19247022e-02 4.12395639e-06]
 [2.50200182e-03 5.95974565e-01 6.88982895e-03 1.48611469e-02
  1.52046839e-03 2.23246516e-05 3.77550483e-01 1.60362382e-04
  5.15544496e-04 3.31728552e-06]
 [9.94613707e-01 7.41545136e-07 6.28760841e-04 7.62155323e-05
  4.51910182e-07 3.84655186e-06 2.92377663e-03 1.66005769e-03
  8.25389725e-05 9.90487933e-06]
 [8.23299866e-03 7.03133002e-04 4.30546217e-02 5.91966212e-02
  8.59538078e-01 9.54521820e-05 6.75338320e-03 4.18034149e-03
  1.82398222e-02 5.46405317e-06]
 [3.93521514e-05 8.05094260e-06 7.86758319e-05 4.02450787e-05
  3.99258715e-05 9.99613583e-01 4.49181789e-05 3.57692588e-05
  3.32891318e-06 9.62358172e-05]
 [6.46398738e-02 1.05187610e-04 2.06559384e-03 2.36271750e-02
  2.02959374e-01 4.31610300e-04 4.91256192e-02 2.55684238e-02
  6.31453574e-01 2.35435436e-05]
 [3.12692206e-03 1.25801889e-02 1.85741093e-02 5.09717828e-03
  1.39112361e-02 2.19145280e-04 9.96380392e-03 1.71764363e-02
  9.19216573e-01 1.34402420e-04]
 [4.71906333e-07 1.15334593e-07 9.66971243e-07 9.53821733e-09
  1.34414833e-07 9.70866144e-01 2.51898905e-06 7.56867962e-07
  1.68770910e-07 2.91285682e-02]]
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]
Best Validation Accuracy: 0.7969130247525272
Current Validation Accuracy: 0.7973962400249311
Current Validation Loss: 0.0002282461505370479
Saving best model based on accuracy
Saving last model
